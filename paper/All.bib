
@article{aasaPrecedencesSpecificationsImplementations1995,
  title = {Precedences in Specifications and Implementations of Programming Languages},
  author = {Aasa, Annika},
  year = {1995},
  month = may,
  volume = {142},
  pages = {3--26},
  issn = {0304-3975},
  doi = {10.1016/0304-3975(95)90680-J},
  abstract = {Although precedences are often used to resolve ambiguities in programming language descriptions, there has been no parser-independent definition of languages which are generated by grammars with precedence rules. This paper gives such a definition for a subclass of context-free grammars. The definition is shown to be equivalent to the implicit definition an operator precedence parser gives. A problem with a language containing infix, prefix and postfix operators of different precedences is that the well-known algorithm, which transforms a grammar with infix operator precedences to an ordinary unambiguous context-free grammar, does not work. This paper gives an algorithm that works also for prefix and postfix operators, and the correctness of it is proved. An application of the algorithm is also presented.},
  journal = {Theoretical Computer Science},
  language = {en},
  number = {1},
  series = {Selected {{Papers}} of the {{Symposium}} on {{Programming Language Implementation}} and {{Logic Programming}}}
}

@inproceedings{adamsComplexityPerformanceParsing2016,
  title = {On the {{Complexity}} and {{Performance}} of {{Parsing}} with {{Derivatives}}},
  booktitle = {Proceedings of the 37th {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Adams, Michael D. and Hollenbeck, Celeste and Might, Matthew},
  year = {2016},
  pages = {224--236},
  publisher = {{ACM}},
  address = {{Santa Barbara, CA, USA}},
  doi = {10.1145/2908080.2908128},
  abstract = {Current algorithms for context-free parsing inflict a trade-off between ease of understanding, ease of implementation, theoretical complexity, and practical performance. No algorithm achieves all of these properties simultaneously. Might et al. introduced parsing with derivatives, which handles arbitrary context-free grammars while being both easy to understand and simple to implement. Despite much initial enthusiasm and a multitude of independent implementations, its worst-case complexity has never been proven to be better than exponential. In fact, high-level arguments claiming it is fundamentally exponential have been advanced and even accepted as part of the folklore. Performance ended up being sluggish in practice, and this sluggishness was taken as informal evidence of exponentiality. In this paper, we reexamine the performance of parsing with derivatives. We have discovered that it is not exponential but, in fact, cubic. Moreover, simple (though perhaps not obvious) modifications to the implementation by Might et al. lead to an implementation that is not only easy to understand but also highly performant in practice.},
  isbn = {978-1-4503-4261-2},
  keywords = {Parsing,Parsing with derivatives,Performance},
  series = {{{PLDI}} '16}
}

@inproceedings{afroozehFasterPracticalGLL2015,
  title = {Faster, {{Practical GLL Parsing}}},
  booktitle = {Compiler {{Construction}}},
  author = {Afroozeh, Ali and Izmaylova, Anastasia},
  editor = {Franke, Bj{\"o}rn},
  year = {2015},
  pages = {89--108},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {Generalized LL (GLL) parsing is an extension of recursivedescent (RD) parsing that supports all context-free grammars in cubic time and space. GLL parsers have the direct relationship with the grammar that RD parsers have, and therefore, compared to GLR, are easier to understand, debug, and extend. This makes GLL parsing attractive for parsing programming languages.In this paper we propose a more efficient Graph-Structured Stack (GSS) for GLL parsing that leads to significant performance improvement. We also discuss a number of optimizations that further improve the performance of GLL. Finally, for practical scannerless parsing of programming languages, we show how common lexical disambiguation filters can be integrated in GLL parsing.Our new formulation of GLL parsing is implemented as part of the Iguana parsing framework. We evaluate the effectiveness of our approach using a highly-ambiguous grammar and grammars of real programming languages. Our results, compared to the original GLL, show a speedup factor of 10 on the highly-ambiguous grammar, and a speedup factor of 1.5, 1.7, and 5.2 on the grammars of Java, C\#, and OCaml, respectively.},
  isbn = {978-3-662-46663-6},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{afroozehSafeSpecificationOperator2013,
  title = {Safe {{Specification}} of {{Operator Precedence Rules}}},
  booktitle = {Software {{Language Engineering}}},
  author = {Afroozeh, Ali and {van den Brand}, Mark and Johnstone, Adrian and Scott, Elizabeth and Vinju, Jurgen},
  editor = {Erwig, Martin and Paige, Richard F. and Van Wyk, Eric},
  year = {2013},
  pages = {137--156},
  publisher = {{Springer International Publishing}},
  abstract = {In this paper we present an approach to specifying operator precedence based on declarative disambiguation constructs and an implementation mechanism based on grammar rewriting. We identify a problem with existing generalized context-free parsing and disambiguation technology: generating a correct parser for a language such as OCaml using declarative precedence specification is not possible without resorting to some manual grammar transformation. Our approach provides a fully declarative solution to operator precedence specification for context-free grammars, is independent of any parsing technology, and is safe in that it guarantees that the language of the resulting grammar will be the same as the language of the specification grammar. We evaluate our new approach by specifying the precedence rules from the OCaml reference manual against the highly ambiguous reference grammar and validate the output of our generated parser.},
  isbn = {978-3-319-02654-1},
  keywords = {Derivation Tree,Operator Precedence,Parse Tree,Precedence Rule,Production Rule},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@book{ahoCompilersPrinciplesTechniques2006,
  title = {Compilers: {{Principles}}, {{Techniques}}, and {{Tools}}},
  shorttitle = {Compilers},
  author = {Aho, Alfred V. and Lam, Monica S. and Sethi, Ravi and Ullman, Jeffrey D.},
  year = {2006},
  month = sep,
  edition = {Second},
  publisher = {{Addison Wesley}},
  address = {{Boston}},
  abstract = {Compilers: Principles, Techniques and Tools, known to professors, students, and developers worldwide as the "Dragon Book," is available in a new edition.~ Every chapter has been completely revised to reflect developments in software engineering, programming languages, and computer architecture that have occurred since 1986, when the last edition published.~ The authors, recognizing that few readers will ever go on to construct a compiler, retain their focus on the broader set of problems faced in software design and software development.},
  isbn = {978-0-321-48681-3},
  language = {English}
}

@inproceedings{alurVisiblyPushdownLanguages2004,
  title = {Visibly {{Pushdown Languages}}},
  booktitle = {Proceedings of the {{Thirty}}-Sixth {{Annual ACM Symposium}} on {{Theory}} of {{Computing}}},
  author = {Alur, Rajeev and Madhusudan, P.},
  year = {2004},
  pages = {202--211},
  publisher = {{ACM}},
  address = {{Chicago, IL, USA}},
  doi = {10.1145/1007352.1007390},
  abstract = {We propose the class of visibly pushdown languages as embeddings of context-free languages that is rich enough to model program analysis questions and yet is tractable and robust like the class of regular languages. In our definition, the input symbol determines when the pushdown automaton can push or pop, and thus the stack depth at every position. We show that the resulting class Vpl of languages is closed under union, intersection, complementation, renaming, concatenation, and Kleene-*, and problems such as inclusion that are undecidable for context-free languages are Exptime-complete for visibly pushdown automata. Our framework explains, unifies, and generalizes many of the decision procedures in the program analysis literature, and allows algorithmic verification of recursive programs with respect to many context-free properties including access control properties via stack inspection and correctness of procedures with respect to pre and post conditions. We demonstrate that the class Vpl is robust by giving two alternative characterizations: a logical characterization using the monadic second order (MSO) theory over words augmented with a binary matching predicate, and a correspondence to regular tree languages. We also consider visibly pushdown languages of infinite words and show that the closure properties, MSO-characterization and the characterization in terms of regular trees carry over. The main difference with respect to the case of finite words turns out to be determinizability: nondeterministic B{\"u}chi visibly pushdown automata are strictly more expressive than deterministic Muller visibly pushdown automata.},
  isbn = {978-1-58113-852-8},
  keywords = {$Ã¸mega$-languages,context-free languages,logic,pushdown automata,regular tree languages,verification},
  series = {{{STOC}} '04}
}

@article{anselPetaBricksLanguageCompiler2009,
  title = {{{PetaBricks}}: A Language and Compiler for Algorithmic Choice},
  shorttitle = {{{PetaBricks}}},
  author = {Ansel, Jason and Chan, Cy and Wong, Yee Lok and Olszewski, Marek and Zhao, Qin and Edelman, Alan and Amarasinghe, Saman},
  year = {2009},
  month = jun,
  volume = {44},
  pages = {38--49},
  issn = {0362-1340},
  doi = {10.1145/1543135.1542481},
  abstract = {It is often impossible to obtain a one-size-fits-all solution for high performance algorithms when considering different choices for data distributions, parallelism, transformations, and blocking. The best solution to these choices is often tightly coupled to different architectures, problem sizes, data, and available system resources. In some cases, completely different algorithms may provide the best performance. Current compiler and programming language techniques are able to change some of these parameters, but today there is no simple way for the programmer to express or the compiler to choose different algorithms to handle different parts of the data. Existing solutions normally can handle only coarse-grained, library level selections or hand coded cutoffs between base cases and recursive cases. We present PetaBricks, a new implicitly parallel language and compiler where having multiple implementations of multiple algorithms to solve a problem is the natural way of programming. We make algorithmic choice a first class construct of the language. Choices are provided in a way that also allows our compiler to tune at a finer granularity. The PetaBricks compiler autotunes programs by making both fine-grained as well as algorithmic choices. Choices also include different automatic parallelization techniques, data distributions, algorithmic parameters, transformations, and blocking. Additionally, we introduce novel techniques to autotune algorithms for different convergence criteria. When choosing between various direct and iterative methods, the PetaBricks compiler is able to tune a program in such a way that delivers near-optimal efficiency for any desired level of accuracy. The compiler has the flexibility of utilizing different convergence criteria for the various components within a single algorithm, providing the user with accuracy choice alongside algorithmic choice.},
  journal = {ACM SIGPLAN Notices},
  keywords = {adaptive,algorithmic choice,autotuning,compiler,implicitly parallel,language},
  number = {6}
}

@inproceedings{antwerpenConstraintLanguageStatic2016,
  title = {A {{Constraint Language}} for {{Static Semantic Analysis Based}} on {{Scope Graphs}}},
  booktitle = {Proceedings of the 2016 {{ACM SIGPLAN Workshop}} on {{Partial Evaluation}} and {{Program Manipulation}}},
  author = {van Antwerpen, Hendrik and N{\'e}ron, Pierre and Tolmach, Andrew and Visser, Eelco and Wachsmuth, Guido},
  year = {2016},
  pages = {49--60},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2847538.2847543},
  abstract = {In previous work, we introduced scope graphs as a formalism for describing program binding structure and performing name resolution in an AST-independent way. In this paper, we show how to use scope graphs to build static semantic analyzers. We use constraints extracted from the AST to specify facts about binding, typing, and initialization. We treat name and type resolution as separate building blocks, but our approach can handle language constructs---such as record field access---for which binding and typing are mutually dependent. We also refine and extend our previous scope graph theory to address practical concerns including ambiguity checking and support for a wider range of scope relationships. We describe the details of constraint generation for a model language that illustrates many of the interesting static analysis issues associated with modules and records.},
  isbn = {978-1-4503-4097-7},
  keywords = {Domain Specific Languages,Language Specification,Meta-Theory,Name Binding,Types},
  series = {{{PEPM}} '16}
}

@inproceedings{augustssonParadiseTwostageDSL2008,
  title = {Paradise: {{A Two}}-Stage {{DSL Embedded}} in {{Haskell}}},
  shorttitle = {Paradise},
  booktitle = {Proceedings of the 13th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Augustsson, Lennart and Mansell, Howard and Sittampalam, Ganesh},
  year = {2008},
  pages = {225--228},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1411204.1411236},
  abstract = {We have implemented a two-stage language, Paradise, for building reusable components which are used to price financial products. Paradise is embedded in Haskell and makes heavy use of type-class based overloading, allowing the second stage to be compiled into a variety of backend platforms. Paradise has enabled us to begin moving away from implementation directly in monolithic Excel spreadsheets and towards a more modular and retargetable approach.},
  isbn = {978-1-59593-919-7},
  keywords = {dsels,Haskell,metaprogramming,paradise},
  series = {{{ICFP}} '08}
}

@article{augustssonParadiseTwostageDSL2008a,
  title = {Paradise: {{A Two}}-Stage {{DSL Embedded}} in {{Haskell}}},
  author = {Augustsson, Lennart and Mansell, Howard and Sittampalam, Ganesh},
  year = {2008},
  month = sep,
  volume = {43},
  pages = {225--228},
  issn = {0362-1340},
  doi = {10.1145/1411203.1411236},
  journal = {SIGPLAN Not.},
  keywords = {dsels,Haskell,metaprogramming,paradise},
  number = {9}
}

@inproceedings{axelssonAnalyzingContextFreeGrammars2008,
  title = {Analyzing {{Context}}-{{Free Grammars Using}} an {{Incremental SAT Solver}}},
  booktitle = {Automata, {{Languages}} and {{Programming}}},
  author = {Axelsson, Roland and Heljanko, Keijo and Lange, Martin},
  editor = {Aceto, Luca and Damg{\aa}rd, Ivan and Goldberg, Leslie Ann and Halld{\'o}rsson, Magn{\'u}s M. and Ing{\'o}lfsd{\'o}ttir, Anna and Walukiewicz, Igor},
  year = {2008},
  pages = {410--422},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {We consider bounded versions of undecidable problems about context-free languages which restrict the domain of words to some finite length: inclusion, intersection, universality, equivalence, and ambiguity. These are in (co)-NP and thus solvable by a reduction to the (un-)satisfiability problem for propositional logic. We present such encodings \textendash{} fully utilizing the power of incrementat SAT solvers \textendash{} prove correctness and validate this approach with benchmarks.},
  isbn = {978-3-540-70583-3},
  keywords = {Ambiguous Word,Conjunctive Normal Form,Parse Tree,Propositional Formula,Word Problem},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{aycockEarlyActionEarley2009,
  title = {Early Action in an {{Earley}} Parser},
  author = {Aycock, John and Borsotti, Angelo},
  year = {2009},
  month = oct,
  volume = {46},
  pages = {549},
  issn = {1432-0525},
  doi = {10.1007/s00236-009-0107-6},
  abstract = {Traditional Earley parsers operate in two phases: first recognizing the input, then constructing the forest of parse trees. Practically speaking, this quirk makes it awkward to use in a compiler-compiler, because semantic actions attached to rules are only executed after the fact. We address this problem by identifying safe Earley sets, points during the recognition phase at which partial parse trees can be constructed; this means that semantic actions may be executed on the fly. A secondary benefit is that Earley sets can be deleted during recognition, resulting in a substantial savings of both space and time.},
  journal = {Acta Informatica},
  keywords = {Causal Link,Early Action,Grammar Rule,Parse Tree,Semantic Action},
  language = {en},
  number = {8}
}

@article{aycockPracticalEarleyParsing2002,
  title = {Practical {{Earley Parsing}}},
  author = {Aycock, John and Horspool, R. Nigel},
  year = {2002},
  volume = {45},
  pages = {620--630},
  issn = {0010-4620},
  doi = {10.1093/comjnl/45.6.620},
  abstract = {Earley's parsing algorithm is a general algorithm, able to handle any context-free grammar. As with most parsing algorithms, however, the presence of grammar rules having empty right-hand sides complicates matters. By analyzing why Earley's algorithm struggles with these grammar rules, we have devised a simple solution to the problem. Our empty-rule solution leads to a new type of finite automaton expressly suited for use in Earley parsers and to a new statement of Earley's algorithm. We show that this new form of Earley parser is much more time efficient in practice than the original.},
  journal = {The Computer Journal},
  keywords = {Algorithms,Article,Automation,Computing Milieux (General) (Ci),Copyrights,Grammars,Handles,Mathematical Analysis,Mathematical Models,Parsers,Parsing Algorithms},
  number = {6}
}

@article{baggeInterfacingConceptsWhy2010,
  title = {Interfacing {{Concepts}}: {{Why Declaration Style Shouldn}}'t {{Matter}}},
  shorttitle = {Interfacing {{Concepts}}},
  author = {Bagge, Anya Helene and Haveraaen, Magne},
  year = {2010},
  month = sep,
  volume = {253},
  pages = {37--50},
  issn = {1571-0661},
  doi = {10.1016/j.entcs.2010.08.030},
  abstract = {A concept (or signature) describes the interface of a set of abstract types by listing the operations that should be supported for those types. When implementing a generic operation, such as sorting, we may then specify requirements such as ``elements must be comparable'' by requiring that the element type models the Comparable concept. We may also use axioms to describe behaviour that should be common to all models of a concept. However, the operations specified by the concept are not always the ones that are best suited for the implementation. For example, numbers and matrices may both be addable, but adding two numbers is conveniently done by using a return value, whereas adding a sparse and a dense matrix is probably best achieved by modifying the dense matrix. In both cases, though, we may want to pretend we're using a simple function with a return value, as this most closely matches the notation we know from mathematics. This paper presents two simple concepts to break the notational tie between implementation and use of an operation: functionalisation, which derives a set of canonical pure functions from a procedure; and mutification, which translates calls using the functionalised declarations into calls to the implemented procedure.},
  journal = {Electronic Notes in Theoretical Computer Science},
  keywords = {axioms,concept-based programming,concepts,functions,imperative vs functional,mutification,procedures},
  language = {en},
  number = {7},
  series = {Proceedings of the {{Ninth Workshop}} on {{Language Descriptions Tools}} and {{Applications}} ({{LDTA}} 2009)}
}

@phdthesis{bastenAmbiguityDetectionProgramming2011,
  title = {Ambiguity {{Detection}} for {{Programming Language Grammars}}},
  author = {Basten, Bas},
  year = {2011},
  month = dec,
  abstract = {Context-free grammars are the most suitable and most widely used method for describing the syntax of programming languages. They can be used to generate parsers, which transform a piece of source code into a tree-shaped representation of the code's syntactic structure. These parse trees can then be used for further processing or analysis of the source text. In this sense, grammars form the basis of many engineering and reverse engineering applications, like compilers, interpreters and tools for software analysis and transformation. Unfortunately, context-free grammars have the undesirable property that they can be ambiguous, which can seriously hamper their applicability. A grammar is ambiguous if at least one sentence in its language has more than one valid parse tree. Since the parse tree of a sentence is often used to infer its semantics, an ambiguous sentence can have multiple meanings. For programming languages this is almost always unintended. Ambiguity can therefore be seen as a grammar bug. A specific category of context-free grammars that is particularly sensitive to ambiguity are character-level grammars, which are used to generate scannerless parsers. Unlike traditional token-based grammars, character-level grammars include the full lexical definition of their language. This has the advantage that a language can be specified in a single formalism, and that no separate lexer or scanner phase is necessary in the parser. However, the absence of a scanner does require some additional lexical disambiguation. Character-level grammars can therefore be annotated with special disambiguation declarations to specify which parse trees to discard in case of ambiguity. Unfortunately, it is very hard to determine whether all ambiguities have been covered. The task of searching for ambiguities in a grammar is very complex and time consuming, and is therefore best automated. Since the invention of context-free grammars, several ambiguity detection methods have been developed to this end. However, the ambiguity problem for context-free grammars is undecidable in general, so the perfect detection method cannot exist. This implies a trade-off between accuracy and termination. Methods that apply exhaustive searching are able to correctly find all ambiguities, but they might never terminate. On the other hand, approximative search techniques do always produce an ambiguity report, but these might contain false positives or false negatives. Nevertheless, the fact that every method has flaws does not mean that ambiguity detection cannot be useful in practice. This thesis investigates ambiguity detection with the aim of checking grammars for programming languages. The challenge is to improve upon the state-of-the-art, by finding accurate enough methods that scale to realistic grammars. First we evaluate existing methods with a set of criteria for practical usability. Then we present various improvements to ambiguity detection in the areas of accuracy, performance and report quality. The main contributions of this thesis are two novel techniques. The first is an ambi- guity detection method that applies both exhaustive and approximative searching, called AMBIDEXTER. The key ingredient of AMBIDEXTER is a grammar filtering technique that can remove harmless production rules from a grammar. A production rule is harmless if it does not contribute to any ambiguity in the grammar. Any found harmless rules can therefore safely be removed. This results in a smaller grammar that still contains the same ambiguities as the original one. However, it can now be searched with exhaustive techniques in less time. The grammar filtering technique is formally proven correct, and experimentally validated. A prototype implementation is applied to a series of programming language grammars, and the performance of exhaustive detection methods are measured before and after filtering. The results show that a small investment in filtering time can substantially reduce the run-time of exhaustive searching, sometimes with several orders of magnitude. After this evaluation on token-based grammars, the grammar filtering technique is extended for use with character-level grammars. The extensions deal with the increased complexity of these grammars, as well as their disambiguation declarations. This enables the detection of productions that are harmless due to disambiguation. The extentions are experimentally validated on another set of programming language grammars from practice, with similar results as before. Measurements show that, even though character-level grammars are more expensive to filter, the investment is still very worthwhile. Exhaustive search times were again reduced substantially. The second main contribution of this thesis is DR. AMBIGUITY, an expert system to help grammar developers to understand and solve found ambiguities. If applied to an ambiguous sentence, DR. AMBIGUITY analyzes the causes of the ambiguity and proposes a number of applicable solutions. A prototype implementation is presented and evaluated with a mature Java grammar. After removing disambiguation declarations from the grammar we analyze sentences that have become ambiguous by this removal. The results show that in all cases the removed filter is proposed by DR. AMBIGUITY as a possible cure for the ambiguity. Concluding, this thesis improves ambiguity detection with two novel methods. The first is the ambiguity detection method AMBIDEXTER that applies grammar filtering to substantially speed up exhaustive searching. The second is the expert system DR. AMBIGUITY that automatically analyzes found ambiguities and proposes applicable cures. The results obtained with both methods show that automatic ambiguity detection is now ready for realistic programming language grammars.},
  hal_id = {tel-00644079},
  hal_version = {v1},
  keywords = {ambiguity detection,context-free grammars,programming languages,scannerless,static analysis},
  school = {Universiteit van Amsterdam}
}

@inproceedings{bjesseLavaHardwareDesign1998,
  title = {Lava: {{Hardware Design}} in {{Haskell}}},
  shorttitle = {Lava},
  booktitle = {Proceedings of the {{Third ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Bjesse, Per and Claessen, Koen and Sheeran, Mary and Singh, Satnam},
  year = {1998},
  pages = {174--184},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/289423.289440},
  abstract = {Lava is a tool to assist circuit designers in specifying, designing, verifying and implementing hardware. It is a collection of Haskell modules. The system design exploits functional programming language features, such as monads and type classes, to provide multiple interpretations of circuit descriptions. These interpretations implement standard circuit analyses such as simulation, formal verification and the generation of code for the production of real circuits.Lava also uses polymorphism and higher order functions to provide more abstract and general descriptions than are possible in traditional hardware description languages. Two Fast Fourier Transform circuit examples illustrate this.},
  isbn = {978-1-58113-024-9},
  series = {{{ICFP}} '98}
}

@inproceedings{brabrandAnalyzingAmbiguityContextFree2007,
  title = {Analyzing {{Ambiguity}} of {{Context}}-{{Free Grammars}}},
  booktitle = {Implementation and {{Application}} of {{Automata}}},
  author = {Brabrand, Claus and Giegerich, Robert and M{\o}ller, Anders},
  editor = {Holub, Jan and {\v Z}{\v d}{\'a}rek, Jan},
  year = {2007},
  pages = {214--225},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {It has been known since 1962 that the ambiguity problem for context-free grammars is undecidable. Ambiguity in context-free grammars is a recurring problem in language design and parser generation, as well as in applications where grammars are used as models of real-world physical structures.We observe that there is a simple linguistic characterization of the grammar ambiguity problem, and we show how to exploit this to conservatively approximate the problem based on local regular approximations and grammar unfoldings. As an application, we consider grammars that occur in RNA analysis in bioinformatics, and we demonstrate that our static analysis of context-free grammars is sufficiently precise and efficient to be practically useful.},
  isbn = {978-3-540-76336-9},
  keywords = {CFG ambiguity,regular approximation,RNA analysis},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{brabrandTypedUnambiguousPattern2010,
  title = {Typed and {{Unambiguous Pattern Matching}} on {{Strings Using Regular Expressions}}},
  booktitle = {Proceedings of the 12th {{International ACM SIGPLAN Symposium}} on {{Principles}} and {{Practice}} of {{Declarative Programming}}},
  author = {Brabrand, Claus and Thomsen, Jakob G.},
  year = {2010},
  pages = {243--254},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1836089.1836120},
  abstract = {We show how to achieve typed and unambiguous declarative pattern matching on strings using regular expressions extended with a simple recording operator. We give a characterization of ambiguity of regular expressions that leads to a sound and complete static analysis. The analysis is capable of pinpointing all ambiguities in terms of the structure of the regular expression and report shortest ambiguous strings. We also show how pattern matching can be integrated into statically typed programming languages for deconstructing strings and reproducing typed and structured values. We validate our approach by giving a full implementation of the approach presented in this paper. The resulting tool, reg-exp-rec, adds typed and unambiguous pattern matching to Java in a standalone and non-intrusive manner. We evaluate the approach using several realistic examples.},
  isbn = {978-1-4503-0132-9},
  keywords = {ambiguity,disambiguation,parsing,pattern matching,regular expressions,static analysis,type inference},
  series = {{{PPDP}} '10}
}

@article{bravenboerStrategoXT172008,
  title = {Stratego/{{XT}} 0.17. {{A Language}} and {{Toolset}} for {{Program Transformation}}},
  author = {Bravenboer, Martin and Kalleberg, Karl Trygve and Vermaas, Rob and Visser, Eelco},
  year = {2008},
  month = jun,
  volume = {72},
  pages = {52--70},
  issn = {0167-6423},
  doi = {10.1016/j.scico.2007.11.003},
  abstract = {Stratego/XT is a language and toolset for program transformation. The Stratego language provides rewrite rules for expressing basic transformations, programmable rewriting strategies for controlling the application of rules, concrete syntax for expressing the patterns of rules in the syntax of the object language, and dynamic rewrite rules for expressing context-sensitive transformations, thus supporting the development of transformation components at a high level of abstraction. The XT toolset offers a collection of flexible, reusable transformation components, and tools for generating such components from declarative specifications. Complete program transformation systems are composed from these components. This paper gives an overview of Stratego/XT 0.17, including a description of the Stratego language and XT transformation tools; a discussion of the implementation techniques and software engineering process; and a description of applications built with Stratego/XT.},
  journal = {Sci. Comput. Program.},
  keywords = {Concrete syntax,Dynamic rewrite rules,Program transformation,Rewrite rules,Rewriting strategies,Stratego,Stratego/XT},
  number = {1-2}
}

@inproceedings{bromanGraduallyTypedSymbolic2018,
  title = {Gradually {{Typed Symbolic Expressions}}},
  booktitle = {Proceedings of the {{ACM SIGPLAN Workshop}} on {{Partial Evaluation}} and {{Program Manipulation}}},
  author = {Broman, David and Siek, Jeremy G.},
  year = {2018},
  pages = {15--29},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3162068},
  abstract = {Embedding a domain-specific language (DSL) in a general purpose host language is an efficient way to develop a new DSL. Various kinds of languages and paradigms can be used as host languages, including object-oriented, functional, statically typed, and dynamically typed variants, all having their pros and cons. For deep embedding, statically typed languages enable early checking and potentially good DSL error messages, instead of reporting runtime errors. Dynamically typed languages, on the other hand, enable flexible transformations, thus avoiding extensive boilerplate code. In this paper, we introduce the concept of gradually typed symbolic expressions that mix static and dynamic typing for symbolic data. The key idea is to combine the strengths of dynamic and static typing in the context of deep embedding of DSLs. We define a gradually typed calculus {$\lambda{}<\star{}>$}, formalize its type system and dynamic semantics, and prove type safety. We introduce a host language called Modelyze that is based on {$\lambda{}<\star{}>$}, and evaluate the approach by embedding a series of equation-based domain-specific modeling languages, all within the domain of physical modeling and simulation.},
  isbn = {978-1-4503-5587-2},
  keywords = {DSL,Symbolic expressions,Type systems},
  series = {{{PEPM}} '18}
}

@techreport{bromanModelyzeGraduallyTyped2012,
  title = {Modelyze: A {{Gradually Typed Host Language}} for {{Embedding Equation}}-{{Based Modeling Languages}}},
  author = {Broman, David and Siek, Jeremy G.},
  year = {2012},
  month = jun,
  institution = {{EECS Department, University of California, Berkeley}},
  number = {UCB/EECS-2012-173}
}

@article{cantorAmbiguityProblemBackus1962,
  title = {On {{The Ambiguity Problem}} of {{Backus Systems}}},
  author = {Cantor, David G.},
  year = {1962},
  month = oct,
  volume = {9},
  pages = {477--479},
  issn = {0004-5411},
  doi = {10.1145/321138.321145},
  abstract = {Backus [1] has developed an elegant method of defining well-formed formulas for computer languages such as ALGOL. It consists of (our notation is slightly different from that of Backus):    A finite alphabet: a1, a2, \ldots, at; Predicates: P1, P2, \ldots, P{$\epsilon$}; Productions, either of the form (a) aj {$\in$} Pi;},
  journal = {Journal of the ACM},
  number = {4}
}

@article{caralpTrimmingVisiblyPushdown2015,
  title = {Trimming Visibly Pushdown Automata},
  author = {Caralp, Mathieu and Reynier, Pierre-Alain and Talbot, Jean-Marc},
  year = {2015},
  month = may,
  volume = {578},
  pages = {13--29},
  issn = {0304-3975},
  doi = {10.1016/j.tcs.2015.01.018},
  abstract = {We study the problem of trimming visibly pushdown automata (VPA). We first describe a polynomial time procedure which, given a visibly pushdown automaton that accepts only well-nested words, returns an equivalent visibly pushdown automaton that is trimmed. We then show how this procedure can be lifted to the setting of arbitrary VPA. Furthermore, we present a way of building, given a VPA, an equivalent VPA which is both deterministic and trimmed. Last, our trimming procedures can be applied to weighted VPA.},
  journal = {Theoretical Computer Science},
  keywords = {Polynomial Time,Polynomial Time Complexity,Pushdown Automaton,Return Transition,Tree Automaton,Trimming,Visibly pushdown automata},
  language = {en},
  series = {Implementation and {{Application}} of {{Automata}}}
}

@inproceedings{castagnaSettheoreticTypesPolymorphic2016,
  title = {Set-Theoretic Types for Polymorphic Variants},
  booktitle = {Proceedings of the 21st {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Castagna, Giuseppe and Petrucciani, Tommaso and Nguye\^\~n, Kim},
  year = {2016},
  month = sep,
  pages = {378--391},
  publisher = {{Association for Computing Machinery}},
  address = {{Nara, Japan}},
  doi = {10.1145/2951913.2951928},
  abstract = {Polymorphic variants are a useful feature of the OCaml language whose current definition and implementation rely on kinding constraints to simulate a subtyping relation via unification. This yields an awkward formalization and results in a type system whose behaviour is in some cases unintuitive and/or unduly restrictive. In this work, we present an alternative formalization of polymorphic variants, based on set-theoretic types and subtyping, that yields a cleaner and more streamlined system. Our formalization is more expressive than the current one (it types more programs while preserving type safety), it can internalize some meta-theoretic properties, and it removes some pathological cases of the current implementation resulting in a more intuitive and, thus, predictable type system. More generally, this work shows how to add full-fledged union types to functional languages of the ML family that usually rely on the Hindley-Milner type system. As an aside, our system also improves the theory of semantic subtyping, notably by proving completeness for the type reconstruction algorithm.},
  isbn = {978-1-4503-4219-3},
  keywords = {type constraints,Type reconstruction,union types},
  series = {{{ICFP}} 2016}
}

@incollection{chomskyAlgebraicTheoryContextFree1963,
  title = {The {{Algebraic Theory}} of {{Context}}-{{Free Languages}}*},
  booktitle = {Studies in {{Logic}} and the {{Foundations}} of {{Mathematics}}},
  author = {Chomsky, N. and Sch{\"u}tzenberger, M. P.},
  editor = {Braffort, P. and Hirschberg, D.},
  year = {1963},
  month = jan,
  volume = {35},
  pages = {118--161},
  publisher = {{Elsevier}},
  doi = {10.1016/S0049-237X(08)72023-8},
  abstract = {This chapter discusses the several classes of sentence-generating devices that are closely related, in various ways, to the grammars of both natural languages and artificial languages of various kinds. By a language it simply mean a set of strings in some finite set V of symbols called the vocabulary of the language. By a grammar a set of rules that give a recursive enumeration of the strings belonging to the language. It can be said that the grammar generates these strings. The chapter discusses the aspect of the structural description of a sentence, namely, its subdivision into phrases belonging to various categories. A major concern of the general theory of natural languages is to define the class of possible strings; the class of possible grammars; the class of possible structural descriptions; a procedure for assigning structural descriptions to sentences, given a grammar; and to do all of this in such a way that the structural description assigned to a sentence by the grammar of a natural language will provide the basis for explaining how a speaker of this language would understand this sentence.},
  series = {Computer {{Programming}} and {{Formal Systems}}}
}

@inproceedings{chouAutomaticGenerationEfficient2020,
  title = {Automatic Generation of Efficient Sparse Tensor Format Conversion Routines},
  booktitle = {Proceedings of the 41st {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Chou, Stephen and Kjolstad, Fredrik and Amarasinghe, Saman},
  year = {2020},
  month = jun,
  pages = {823--838},
  publisher = {{Association for Computing Machinery}},
  address = {{London, UK}},
  doi = {10.1145/3385412.3385963},
  abstract = {This paper shows how to generate code that efficiently converts sparse tensors between disparate storage formats (data layouts) such as CSR, DIA, ELL, and many others. We decompose sparse tensor conversion into three logical phases: coordinate remapping, analysis, and assembly. We then develop a language that precisely describes how different formats group together and order a tensor's nonzeros in memory. This lets a compiler emit code that performs complex remappings of nonzeros when converting between formats. We also develop a query language that can extract statistics about sparse tensors, and we show how to emit efficient analysis code that computes such queries. Finally, we define an abstract interface that captures how data structures for storing a tensor can be efficiently assembled given specific statistics about the tensor. Disparate formats can implement this common interface, thus letting a compiler emit optimized sparse tensor conversion code for arbitrary combinations of many formats without hard-coding for any specific combination. Our evaluation shows that the technique generates sparse tensor conversion routines with performance between 1.00 and 2.01\texttimes{} that of hand-optimized versions in SPARSKIT and Intel MKL, two popular sparse linear algebra libraries. And by emitting code that avoids materializing temporaries, which both libraries need for many combinations of source and target formats, our technique outperforms those libraries by 1.78 to 4.01\texttimes{} for CSC/COO to DIA/ELL conversion.},
  isbn = {978-1-4503-7613-6},
  keywords = {attribute query language,coordinate remapping notation,sparse tensor algebra,sparse tensor assembly,sparse tensor conversion,sparse tensor formats},
  series = {{{PLDI}} 2020}
}

@inproceedings{clingerMacrosThatWork1991,
  title = {Macros {{That Work}}},
  booktitle = {Proceedings of the 18th {{ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Clinger, William and Rees, Jonathan},
  year = {1991},
  pages = {155--162},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/99583.99607},
  isbn = {978-0-89791-419-2},
  series = {{{POPL}} '91}
}

@inproceedings{coenEfficientAmbiguousParsing2004,
  title = {Efficient {{Ambiguous Parsing}} of {{Mathematical Formulae}}},
  booktitle = {Mathematical {{Knowledge Management}}},
  author = {Coen, Claudio Sacerdoti and Zacchiroli, Stefano},
  editor = {Asperti, Andrea and Bancerek, Grzegorz and Trybulec, Andrzej},
  year = {2004},
  pages = {347--362},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-27818-4_25},
  abstract = {Mathematical notation has the characteristic of being ambiguous: operators can be overloaded and information that can be deduced is often omitted. Mathematicians are used to this ambiguity and can easily disambiguate a formula making use of the context and of their ability to find the right interpretation.Software applications that have to deal with formulae usually avoid these issues by fixing an unambiguous input notation. This solution is annoying for mathematicians because of the resulting tricky syntaxes and becomes a show stopper to the simultaneous adoption of tools characterized by different input languages.In this paper we present an efficient algorithm suitable for ambiguous parsing of mathematical formulae. The only requirement of the algorithm is the existence of a ``validity'' predicate over abstract syntax trees of incomplete formulae with placeholders. This requirement can be easily fulfilled in the applicative area of interactive proof assistants, and in several other areas of Mathematical Knowledge Management.},
  isbn = {978-3-540-27818-4},
  keywords = {Abstract Data Type,Mathematical Formula,Proof Assistant,Syntax Tree,Type Theory},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@book{comonTreeAutomataTechniques2007,
  title = {Tree {{Automata Techniques}} and {{Applications}}},
  author = {Comon, H. and Dauchet, M. and Gilleron, R. and L{\"o}ding, C. and Jacquemard, F. and Lugiez, D. and Tison, S. and Tommasi, M.},
  year = {2007},
  howpublished = {Available on: http://www.grappa.univ-lille3.fr/tata},
  note = {release October, 12th 2007}
}

@book{cooperEngineeringCompiler2011,
  title = {Engineering a {{Compiler}}},
  author = {Cooper, Keith and Torczon, Linda},
  year = {2011},
  month = jan,
  edition = {Second},
  publisher = {{Elsevier}},
  abstract = {This entirely revised second edition of Engineering a Compiler is full of technical updates and new material covering the latest developments in compiler technology. In this comprehensive text you will learn important techniques for constructing a modern compiler. Leading educators and researchers Keith Cooper and Linda Torczon combine basic principles with pragmatic insights from their experience building state-of-the-art compilers. They will help you fully understand important techniques such as compilation of imperative and object-oriented languages, construction of static single assignment forms, instruction scheduling, and graph-coloring register allocation.In-depth treatment of algorithms and techniques used in the front end of a modern compilerFocus on code optimization and code generation, the primary areas of recent research and developmentImprovements in presentation including conceptual overviews for each chapter, summaries and review questions for sections, and prominent placement of definitions for new termsExamples drawn from several different programming languages},
  googlebooks = {\_tgh4bgQ6PAC},
  isbn = {978-0-08-091661-3},
  language = {en}
}

@article{cordyTXLSourceTransformation2006,
  title = {The {{TXL}} Source Transformation Language},
  author = {Cordy, James R.},
  year = {2006},
  month = aug,
  volume = {61},
  pages = {190--210},
  issn = {0167-6423},
  doi = {10.1016/j.scico.2006.04.002},
  abstract = {TXL is a special-purpose programming language designed for creating, manipulating and rapidly prototyping language descriptions, tools and applications. TXL is designed to allow explicit programmer control over the interpretation, application, order and backtracking of both parsing and rewriting rules. Using first order functional programming at the higher level and term rewriting at the lower level, TXL provides for flexible programming of traversals, guards, scope of application and parameterized context. This flexibility has allowed TXL users to express and experiment with both new ideas in parsing, such as robust, island and agile parsing, and new paradigms in rewriting, such as XML mark-up, rewriting strategies and contextualized rules, without any change to TXL itself. This paper outlines the history, evolution and concepts of TXL with emphasis on its distinctive style and philosophy, and gives examples of its use in expressing and applying recent new paradigms in language processing.},
  journal = {Science of Computer Programming},
  keywords = {Functional programming,Grammars,Source transformation,Term rewriting},
  language = {en},
  number = {3},
  series = {Special {{Issue}} on {{The Fourth Workshop}} on {{Language Descriptions}}, {{Tools}}, and {{Applications}} ({{LDTA}} '04)}
}

@article{cunhaStronglyTypedRewriting2007,
  title = {Strongly {{Typed Rewriting For Coupled Software Transformation}}},
  author = {Cunha, Alcino and Visser, Joost},
  year = {2007},
  month = apr,
  volume = {174},
  pages = {17--34},
  issn = {1571-0661},
  doi = {10.1016/j.entcs.2006.10.019},
  abstract = {Coupled transformations occur in software evolution when multiple artifacts must be modified in such a way that they remain consistent with each other. An important example involves the coupled transformation of a data type, its instances, and the programs that consume or produce it. Previously, we have provided a formal treatment of transformation of the first two: data types and instances. The treatment involved the construction of type-safe, type-changing strategic rewrite systems. In this paper, we extend our treatment to the transformation of corresponding data processing programs. The key insight underlying the extension is that both data migration functions and data processors can be represented type-safely by a generalized abstract data type (GADT). These representations are then subjected to program calculation rules, harnessed in type-safe, type-preserving strategic rewrite systems. For ease of calculation, we use point-free representations and corresponding calculation rules. Thus, coupled transformations are carried out in two steps. First, a type-changing rewrite system is applied to a source type to obtain a target type together with (representations of) migration functions between source and target. Then, a type-preserving rewrite system is applied to the composition of a migration function and a data processor on the source (or target) type to obtain a data processor on the target (or source) type. All rewrites are type-safe.},
  journal = {Electronic Notes in Theoretical Computer Science},
  keywords = {data refinement,generalized abstract datatypes,Program transformation,strategic programming,term rewriting},
  language = {en},
  number = {1},
  series = {Proceedings of the 7th {{International Workshop}} on {{Rule Based Programming}} ({{RULE}} 2006)}
}

@inproceedings{danielssonParsingMixfixOperators2011,
  title = {Parsing {{Mixfix Operators}}},
  booktitle = {Implementation and {{Application}} of {{Functional Languages}}},
  author = {Danielsson, Nils Anders and Norell, Ulf},
  editor = {Scholz, Sven-Bodo and Chitil, Olaf},
  year = {2011},
  pages = {80--99},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {A simple grammar scheme for expressions containing mixfix operators is presented. The scheme is parameterised by a precedence relation which is only restricted to be a directed acyclic graph; this makes it possible to build up precedence relations in a modular way. Efficient and simple implementations of parsers for languages with user-defined mixfix operators, based on the grammar scheme, are also discussed. In the future we plan to replace the support for mixfix operators in the language Agda with a grammar scheme and an implementation based on this work.},
  isbn = {978-3-642-24452-0},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{danvyDefunctionalizationWork2001,
  title = {Defunctionalization at {{Work}}},
  booktitle = {Proceedings of the 3rd {{ACM SIGPLAN International Conference}} on {{Principles}} and {{Practice}} of {{Declarative Programming}}},
  author = {Danvy, Olivier and Nielsen, Lasse R.},
  year = {2001},
  pages = {162--174},
  publisher = {{ACM}},
  address = {{Florence, Italy}},
  doi = {10.1145/773184.773202},
  abstract = {Reynolds's defunctionalization technique is a whole-program transformation from higher-order to first-order functional programs. We study practical applications of this transformation and uncover new connections between seemingly unrelated higher-order and first-order specifications and between their correctness proofs. Defunctionalization therefore appearsboth as a springboard for rev ealing new connections and as a bridge for transferring existing results between the first-order world and the higher-order world.},
  isbn = {978-1-58113-388-2},
  keywords = {church encoding,closure conversion,continuation-passing style (CPS),continuations,CPS transformation,defunctionalization,direct-style transformation,first-order programs,higher-order programs,lambda-lifting,ML,regular expressions,Scheme,supercombinator conversion,syntactic theories},
  series = {{{PPDP}} '01}
}

@article{danvyRefunctionalizationWork2009,
  title = {Refunctionalization at Work},
  author = {Danvy, Olivier and Millikin, Kevin},
  year = {2009},
  month = jun,
  volume = {74},
  pages = {534--549},
  issn = {0167-6423},
  doi = {10.1016/j.scico.2007.10.007},
  abstract = {We present the left inverse of Reynolds' defunctionalization and we show its relevance to programming and to programming languages. We propose two methods to transform a program that is almost in defunctionalized form into one that is actually in defunctionalized form, and we illustrate them with a recognizer for Dyck words and with Dijkstra's shunting-yard algorithm.},
  journal = {Science of Computer Programming},
  keywords = {Abstract machines,Continuation-passing style (CPS),Continuations,Defunctionalization,Refunctionalization,Shunting-yard algorithm},
  language = {en},
  number = {8},
  series = {Special {{Issue}} on {{Mathematics}} of {{Program Construction}} ({{MPC}} 2006)}
}

@inproceedings{doczkalConstructiveTheoryRegular2013,
  title = {A {{Constructive Theory}} of {{Regular Languages}} in {{Coq}}},
  booktitle = {Certified {{Programs}} and {{Proofs}}},
  author = {Doczkal, Christian and Kaiser, Jan-Oliver and Smolka, Gert},
  editor = {Gonthier, Georges and Norrish, Michael},
  year = {2013},
  pages = {82--97},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  abstract = {We present a formal constructive theory of regular languages consisting of about 1400 lines of Coq/Ssreflect. As representations we consider regular expressions, deterministic and nondeterministic automata, and Myhill and Nerode partitions. We construct computable functions translating between these representations and show that equivalence of representations is decidable. We also establish the usual closure properties, give a minimization algorithm for DFAs, and prove that minimal DFAs are unique up to state renaming. Our development profits much from Ssreflect's support for finite types and graphs.},
  isbn = {978-3-319-03545-1},
  keywords = {Coq,finite automata,Myhill-Nerode,regular expressions,regular languages,Ssreflect},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@book{dolanAlgebraicSubtypingDistinguished2017,
  title = {Algebraic Subtyping: {{Distinguished Dissertation}} 2017},
  shorttitle = {Algebraic Subtyping},
  author = {Dolan, Stephen},
  year = {2017},
  publisher = {{BCS}},
  address = {{Swindon, GBR}},
  abstract = {Type inference gives programmers the benefit of static, compile-time type checking without the cost of manually specifying types, and has long been a standard feature of functional programming languages. However, it has proven difficult to integrate type inference with subtyping, since the unification engine at the core of classical type inference accepts only equations, not subtyping constraints. This thesis presents a type system combining ML-style parametric polymorphism and subtyping, with type inference, principal types, and decidable type subsumption. Type inference is based on biunification, an analogue of unification that works with subtyping constraints. Making this possible are several contributions, beginning with the notion of an extensible type system, in which an open world of types is assumed, so that no typeable program becomes untypeable by the addition of new types to the language. While previous formulations of subtyping fail to be extensible, this thesis shows that adopting a more algebraic approach can remedy this. Using such an approach, this thesis develops the theory of biunification, shows how it is used to infer types, and shows how it can be efficiently implemented, exploiting deep connections between the algebra of regular languages and polymorphic subtyping.},
  isbn = {978-1-78017-415-0}
}

@article{dybvigSyntacticAbstractionScheme1993,
  title = {Syntactic Abstraction in Scheme},
  author = {Dybvig, R. Kent and Hieb, Robert and Bruggeman, Carl},
  year = {1993},
  month = dec,
  volume = {5},
  pages = {295--326},
  issn = {1573-0557},
  doi = {10.1007/BF01806308},
  abstract = {Naive program transformations can have surprising effects due to the interaction between introduced identifier references and previously existing identifier bindings, or between introduced bindings and previously existing references. These interactions can result in inadvertent binding, or capturing, of identifiers. A further complication is that transformed programs may have little resemblance to original programs, making correlation of source and object code difficult. This article describes an efficient macro system that prevents inadvertent capturing while maintaining the correlation between source and object code. The macro system allows the programmer to define program transformations using an unrestricted, general-purpose language. Previous approaches to the capturing problem have been inadequate, overly restrictive, or inefficient, and the problem of source-object correlation has been largely unaddressed. The macro system is based on a new algorithm for implementing syntactic transformations and a new representation for syntactic expressions.},
  journal = {LISP and Symbolic Computation},
  keywords = {Hygienic Macros,Macros,Program Transformation,Syntactic Abstraction},
  language = {en},
  number = {4}
}

@article{dyvbigMonadicFrameworkDelimited2007a,
  title = {A Monadic Framework for Delimited Continuations},
  author = {Dyvbig, R. Kent and Jones, Simon Peyton and Sabry, Amr},
  year = {2007},
  month = nov,
  volume = {17},
  pages = {687--730},
  publisher = {{Cambridge University Press}},
  issn = {1469-7653, 0956-7968},
  doi = {10.1017/S0956796807006259},
  abstract = {Delimited continuations are more expressive than traditional abortive continuations and they apparently require a framework beyond traditional continuation-passing style (CPS). We show that this is not the case: standard CPS is sufficient to explain the common control operators for delimited continuations. We demonstrate this fact and present an implementation as a Scheme library. We then investigate a typed account of delimited continuations that makes explicit where control effects can occur. This results in a monadic framework for typed and encapsulated delimited continuations, which we design and implement as a Haskell library.},
  journal = {Journal of Functional Programming},
  language = {en},
  number = {6}
}

@article{earleyAmbiguityPrecedenceSyntax1975,
  title = {Ambiguity and Precedence in Syntax Description},
  author = {Earley, Jay},
  year = {1975},
  month = jun,
  volume = {4},
  pages = {183--192},
  issn = {1432-0525},
  doi = {10.1007/BF00288747},
  abstract = {SummaryThis paper describes a method of syntax description for programming languages which allows one to factor out that part of the description which deals with the relative precedences of syntactic units. This has been found to produce simpler and more flexible syntax descriptions. It is done by allowing the normal part of the description, which is done in BNF, to be ambiguous; these ambiguities are then resolved by a separate part of the description which gives precedence relations between the conflicting productions from the grammar. The method can be used with any left-to-right parser which is capable of detecting ambiguities and recognizing which productions they come from; We have studied its use with an LR(1) parser, and it requires a small and localized addition to the parser to enable it to deal with the precedence relations.},
  journal = {Acta Informatica},
  keywords = {Communication Network,Data Structure,Information System,Information Theory,Operating System},
  language = {en},
  number = {2}
}

@article{earleyEfficientContextfreeParsing1970,
  title = {An {{Efficient Context}}-Free {{Parsing Algorithm}}},
  author = {Earley, Jay},
  year = {1970},
  month = feb,
  volume = {13},
  pages = {94--102},
  issn = {0001-0782},
  doi = {10.1145/362007.362035},
  abstract = {A parsing algorithm which seems to be the most efficient general context-free algorithm known is described. It is similar to both Knuth's LR(k) algorithm and the familiar top-down algorithm. It has a time bound proportional to n3 (where n is the length of the string being parsed) in general; it has an n2 bound for unambiguous grammars; and it runs in linear time on a large class of grammars, which seems to include most practical context-free programming language grammars. In an empirical comparison it appears to be superior to the top-down and bottom-up algorithms studied by Griffiths and Petrick.},
  journal = {Communications of the ACM},
  keywords = {compilers,computational complexity,context-free grammar,parsing,syntax analysis},
  number = {2}
}

@article{ekmanJastAddSystemModular2007,
  title = {The {{JastAdd}} System \textemdash{} Modular Extensible Compiler Construction},
  author = {Ekman, Torbj{\"o}rn and Hedin, G{\"o}rel},
  year = {2007},
  month = dec,
  volume = {69},
  pages = {14--26},
  issn = {0167-6423},
  doi = {10.1016/j.scico.2007.02.003},
  abstract = {The JastAdd system enables modular specifications of extensible compiler tools and languages. Java has been extended with the Rewritable Circular Reference Attributed Grammars formalism that supports modularization and extensibility through several synergistic mechanisms. Object-orientation and static aspect-oriented programming are combined with declarative attributes and context-dependent rewrites to allow highly modular specifications. The techniques have been verified by implementing a full Java 1.4 compiler with modular extensions for non-null types and Java 5 features.},
  journal = {Science of Computer Programming},
  keywords = {Compiler construction,Extensible languages,Modular implementation},
  number = {1},
  series = {Special Issue on {{Experimental Software}} and {{Toolkits}}}
}

@inproceedings{erdwegLayoutSensitiveGeneralizedParsing2013,
  title = {Layout-{{Sensitive Generalized Parsing}}},
  booktitle = {Software {{Language Engineering}}},
  author = {Erdweg, Sebastian and Rendel, Tillmann and K{\"a}stner, Christian and Ostermann, Klaus},
  editor = {Czarnecki, Krzysztof and Hedin, G{\"o}rel},
  year = {2013},
  pages = {244--263},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {The theory of context-free languages is well-understood and context-free parsers can be used as off-the-shelf tools in practice. In particular, to use a context-free parser framework, a user does not need to understand its internals but can specify a language declaratively as a grammar. However, many languages in practice are not context-free. One particularly important class of such languages is layout-sensitive languages, in which the structure of code depends on indentation and whitespace. For example, Python, Haskell, F\#, and Markdown use indentation instead of curly braces to determine the block structure of code. Their parsers (and lexers) are not declaratively specified but hand-tuned to account for layout-sensitivity.To support declarative specifications of layout-sensitive languages, we propose a parsing framework in which a user can annotate layout in a grammar. Annotations take the form of constraints on the relative positioning of tokens in the parsed subtrees. For example, a user can declare that a block consists of statements that all start on the same column. We have integrated layout constraints into SDF and implemented a layout-sensitive generalized parser as an extension of generalized LR parsing. We evaluate the correctness and performance of our parser by parsing 33 290 open-source Haskell files. Layout-sensitive generalized parsing is easy to use, and its performance overhead compared to layout-insensitive parsing is small enough for practical application.},
  isbn = {978-3-642-36089-3},
  keywords = {Abstract Syntax Tree,Curly Brace,Parse Time,Parse Tree,Statement List},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{erdwegStateArtLanguage2013,
  title = {The {{State}} of the {{Art}} in {{Language Workbenches}}},
  booktitle = {Software {{Language Engineering}}},
  author = {Erdweg, Sebastian and {van der Storm}, Tijs and V{\"o}lter, Markus and Boersma, Meinte and Bosman, Remi and Cook, William R. and Gerritsen, Albert and Hulshout, Angelo and Kelly, Steven and Loh, Alex and Konat, Gabri{\"e}l D. P. and Molina, Pedro J. and Palatnik, Martin and Pohjonen, Risto and Schindler, Eugen and Schindler, Klemens and Solmi, Riccardo and Vergu, Vlad A. and Visser, Eelco and {van der Vlist}, Kevin and Wachsmuth, Guido H. and {van der Woning}, Jimi},
  editor = {Erwig, Martin and Paige, Richard F. and Van Wyk, Eric},
  year = {2013},
  pages = {197--217},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-02654-1_11},
  abstract = {Language workbenches are tools that provide high-level mechanisms for the implementation of (domain-specific) languages. Language workbenches are an active area of research that also receives many contributions from industry. To compare and discuss existing language workbenches, the annual Language Workbench Challenge was launched in 2011. Each year, participants are challenged to realize a given domain-specific language with their workbenches as a basis for discussion and comparison. In this paper, we describe the state of the art of language workbenches as observed in the previous editions of the Language Workbench Challenge. In particular, we capture the design space of language workbenches in a feature model and show where in this design space the participants of the 2013 Language Workbench Challenge reside. We compare these workbenches based on a DSL for questionnaires that was realized in all workbenches.},
  isbn = {978-3-319-02654-1},
  keywords = {Design Space,Digital Forensic,Feature Model,Reference Resolution,Software Product Line},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{farrowComposableAttributeGrammars1992,
  title = {Composable {{Attribute Grammars}}: {{Support}} for {{Modularity}} in {{Translator Design}} and {{Implementation}}},
  shorttitle = {Composable {{Attribute Grammars}}},
  booktitle = {Proceedings of the 19th {{ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Farrow, R. and Marlowe, T. J. and Yellin, D. M.},
  year = {1992},
  pages = {223--234},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/143165.143210},
  abstract = {This paper introduces Composable Attribute Grammars (CAGs), a formalism that extends classical attribute grammars to allow for the modular composition of translation specifications and of translators. CAGs bring to complex translator writing systems the same benefits of modularity found in modern programming languages, including comprehensibility, reusability, and incremental meta-compilation.
A CAG is built from several smaller component AGs, each of which solves a particular subproblem, such as name analysis or register allocation. A component AG is based upon a simplified phrase-structure that reflects the properties of its subproblem rather than the phrase-structure of the source language. Different component phrase-structures for various subproblems are combined by mapping them into a phrase-structure for the source language. Both input and output attributes can be associated with the terminal symbols of a component AG. Output attributes enable the results of solving a subproblem to be distributed back to anywhere that originally contributed part of the subproblem, e.g. transparently distributing the results of global name analysis back to every symbolic reference in the source program.
After introducing CAGs by way of an example, we provide a formal definition of CAGs and their semantics. We describe a subclass of CAGs and their semantics. We describe a subclass of CAGs, called separable CAGs, that have favorable implementation properties. We discuss the novel aspects of CAGs, compare them to other proposals for inserting modularity into attribute grammars, and relate our experience using CAGs in the Linguist translator-writing system.},
  isbn = {978-0-89791-453-6},
  series = {{{POPL}} '92}
}

@inproceedings{flattBindingSetsScopes2016,
  title = {Binding {{As Sets}} of {{Scopes}}},
  booktitle = {Proceedings of the 43rd {{Annual ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Flatt, Matthew},
  year = {2016},
  pages = {705--717},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2837614.2837620},
  abstract = {Our new macro expander for Racket builds on a novel approach to hygiene. Instead of basing macro expansion on variable renamings that are mediated by expansion history, our new expander tracks binding through a set of scopes that an identifier acquires from both binding forms and macro expansions. The resulting model of macro expansion is simpler and more uniform than one based on renaming, and it is sufficiently compatible with Racket's old expander to be practical.},
  isbn = {978-1-4503-3549-2},
  keywords = {binding,hygiene,Macros,scope},
  series = {{{POPL}} '16}
}

@article{flattMacrosThatWork2012,
  title = {Macros That {{Work Together}}: {{Compile}}-Time Bindings, Partial Expansion, and Definition Contexts},
  shorttitle = {Macros That {{Work Together}}},
  author = {Flatt, Matthew and Culpepper, Ryan and Darais, David and Findler, Robert Bruce},
  year = {2012},
  month = mar,
  volume = {22},
  pages = {181--216},
  issn = {1469-7653, 0956-7968},
  doi = {10.1017/S0956796812000093},
  abstract = {Racket is a large language that is built mostly within itself. Unlike the usual approach taken by non-Lisp languages, the self-hosting of Racket is not a matter of bootstrapping one implementation through a previous implementation, but instead a matter of building a tower of languages and libraries via macros. The upper layers of the tower include a class system, a component system, pedagogic variants of Scheme, a statically typed dialect of Scheme, and more. The demands of this language-construction effort require a macro system that is substantially more expressive than previous macro systems. In particular, while conventional Scheme macro systems handle stand-alone syntactic forms adequately, they provide weak support for macros that share information or macros that use existing syntactic forms in new contexts. This paper describes and models features of the Racket macro system, including support for general compile-time bindings, sub-form expansion and analysis, and environment management. The presentation assumes a basic familiarity with Lisp-style macros, and it takes for granted the need for macros that respect lexical scope. The model, however, strips away the pattern and template system that is normally associated with Scheme macros, isolating a core that is simpler, can support pattern and template forms themselves as macros, and generalizes naturally to Racket's other extensions.},
  journal = {Journal of Functional Programming},
  language = {en},
  number = {2}
}

@techreport{flattReferenceRacket2010,
  title = {Reference: {{Racket}}},
  author = {Flatt, Matthew and {PLT}},
  year = {2010},
  institution = {{PLT Design Inc.}},
  number = {PLT-TR-2010-1}
}

@inproceedings{fordParsingExpressionGrammars2004,
  title = {Parsing {{Expression Grammars}}: {{A Recognition}}-Based {{Syntactic Foundation}}},
  shorttitle = {Parsing {{Expression Grammars}}},
  booktitle = {Proceedings of the 31st {{ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Ford, Bryan},
  year = {2004},
  pages = {111--122},
  publisher = {{ACM}},
  address = {{Venice, Italy}},
  doi = {10.1145/964001.964011},
  abstract = {For decades we have been using Chomsky's generative system of grammars, particularly context-free grammars (CFGs) and regular expressions (REs), to express the syntax of programming languages and protocols. The power of generative grammars to express ambiguity is crucial to their original purpose of modelling natural languages, but this very power makes it unnecessarily difficult both to express and to parse machine-oriented languages using CFGs. Parsing Expression Grammars (PEGs) provide an alternative, recognition-based formal foundation for describing machine-oriented syntax, which solves the ambiguity problem by not introducing ambiguity in the first place. Where CFGs express nondeterministic choice between alternatives, PEGs instead use prioritized choice. PEGs address frequently felt expressiveness limitations of CFGs and REs, simplifying syntax definitions and making it unnecessary to separate their lexical and hierarchical components. A linear-time parser can be built for any PEG, avoiding both the complexity and fickleness of LR parsers and the inefficiency of generalized CFG parsing. While PEGs provide a rich set of operators for constructing grammars, they are reducible to two minimal recognition schemas developed around 1970, TS/TDPL and gTS/GTDPL, which are here proven equivalent in effective recognition power.},
  isbn = {978-1-58113-729-3},
  keywords = {BNF,context-free grammars,GTDPL,lexical analysis,packrat parsing,parsing expression grammars,regular expressions,scannerless parsing,syntactic predicates,TDPL,unified grammars},
  series = {{{POPL}} '04}
}

@article{ginsburgAmbiguityContextFree1966,
  title = {Ambiguity in {{Context Free Languages}}},
  author = {Ginsburg, Seymour and Ullian, Joseph},
  year = {1966},
  month = jan,
  volume = {13},
  pages = {62--89},
  issn = {0004-5411},
  doi = {10.1145/321312.321318},
  abstract = {Four principal results about ambiguity in languages (i.e., context free languages) are proved. It is first shown that the problem of determining whether an arbitrary language is inherently ambiguous is recursively unsolvable. Then a decision procedure is presented for determining whether an arbitrary bounded grammar is ambiguous. Next, a necessary and sufficient algebraic condition is given for a bounded language to be inherently ambiguous. Finally, it is shown that no language contained in w1*w2*, each w1 a word, is inherently ambiguous.},
  journal = {Journal of the ACM},
  number = {1}
}

@inproceedings{giorgidzeEmbeddingFunctionalHybrid2011a,
  title = {Embedding a {{Functional Hybrid Modelling Language}} in {{Haskell}}},
  booktitle = {Implementation and {{Application}} of {{Functional Languages}}},
  author = {Giorgidze, George and Nilsson, Henrik},
  editor = {Scholz, Sven-Bodo and Chitil, Olaf},
  year = {2011},
  pages = {138--155},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  abstract = {In this paper we present the first investigation into the implementation of a Functional Hybrid Modelling language for non-causal modelling and simulation of physical systems. In particular, we present a simple way to handle connect constructs: a facility for composing model fragments present in some form in most non-causal modelling languages. Our implementation is realised as a domain-specific language embedded in Haskell. The method of embedding employs quasiquoting, thus demonstrating the effectiveness of this approach for languages that are not suitable for embedding in more traditional ways. Our implementation is available on-line, and thus the first publicly available prototype implementation of a Functional Hybrid Modelling language.},
  isbn = {978-3-642-24452-0},
  keywords = {Abstract Syntax,Functional Programming,Modelling Language,Signal Function,Signal Relation},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{heeringSyntaxDefinitionFormalism1989,
  title = {The {{Syntax Definition Formalism SDF}}\textemdash{{Reference Manual}}\textemdash{}},
  author = {Heering, J. and Hendriks, P. R. H. and Klint, P. and Rekers, J.},
  year = {1989},
  month = nov,
  volume = {24},
  pages = {43--75},
  issn = {0362-1340},
  doi = {10.1145/71605.71607},
  abstract = {SDF is a formalism for the definition of syntax which is comparable to BNF in some respects, but has a wider scope in that it also covers the definition of lexical and abstract syntax. Its design and implementation are tailored towards the language designer who wants to develop new languages as well as implement existing ones in a highly interactive manner. It emphasizes compactness of syntax definitions by offering (a) a standard interface between lexical and context-free syntax; (b) a standard correspondence between context-free and abstract syntax; (c) powerful disambiguation and list constructs; and (d) an efficient incremental implementation which accepts arbitrary context-free syntax definitions. SDF can be combined with a variety of programming and specification languages. In this way these obtain fully general user-definable syntax.},
  journal = {SIGPLAN Not.},
  number = {11}
}

@inproceedings{hermanTheoryHygienicMacros2008,
  title = {A {{Theory}} of {{Hygienic Macros}}},
  booktitle = {Programming {{Languages}} and {{Systems}}},
  author = {Herman, David and Wand, Mitchell},
  editor = {Drossopoulou, Sophia},
  year = {2008},
  pages = {48--62},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {Hygienic macro systems, such as Scheme's, automatically rename variables to prevent unintentional variable capture\textemdash{}in short, they ``just work.'' Yet hygiene has never been formally presented as a specification rather than an algorithm. According to folklore, the definition of hygienic macro expansion hinges on the preservation of alpha-equivalence. But the only known notion of alpha-equivalence for programs with macros depends on the results of macro expansion! We break this circularity by introducing explicit binding specifications into the syntax of macro definitions, permitting a definition of alpha-equivalence independent of expansion. We define a semantics for a first-order subset of Scheme-like macros and prove hygiene as a consequence of confluence.},
  isbn = {978-3-540-78739-6},
  keywords = {Core Form,Pattern Variable,Scheme Program,Shape Type,Type Annotation},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{hermanTheoryTypedHygienic2010,
  title = {A {{Theory}} of {{Typed Hygienic Macros}}},
  author = {Herman, David and Wand, Mitchell},
  year = {2010},
  volume = {4960},
  pages = {48},
  issn = {03029743},
  doi = {10.1007/978-3-540-78739-6_4},
  abstract = {We present the {$\lambda$}m-calculus, a semantics for a language of hygienic macros with a non-trivial theory. Unlike Scheme, where programs must be macro- expanded to be analyzed, our semantics admits reasoning about programs as they appear to programmers. Our contributions include a semantics of hygienic macro expansion, a formal definition of {$\alpha$}-equivalence that is independent of expansion, and a proof that expansion preserves {$\alpha$}-equivalence. The key technical component of our language is a type system similar to Culpepper and Felleisens shape types, but with the novel contribution of binding signature types, which specify the bindings and scope of a macros arguments.},
  journal = {Proceedings of the 17th European Symposium on Programming}
}

@inproceedings{hickeyClojureProgrammingLanguage2008,
  title = {The {{Clojure Programming Language}}},
  booktitle = {Proceedings of the 2008 {{Symposium}} on {{Dynamic Languages}}},
  author = {Hickey, Rich},
  year = {2008},
  pages = {1:1--1:1},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1408681.1408682},
  abstract = {Customers and stakeholders have substantial investments in, and are comfortable with the performance, security and stability of, industry-standard platforms like the JVM and CLR. While Java and C\# developers on those platforms may envy the succinctness, flexibility and productivity of dynamic languages, they have concerns about running on customer-approved infrastructure, access to their existing code base and libraries, and performance. In addition, they face ongoing problems dealing with concurrency using native threads and locking. Clojure is an effort in pragmatic dynamic language design in this context. It endeavors to be a general-purpose language suitable in those areas where Java is suitable. It reflects the reality that, for the concurrent programming future, pervasive, unmoderated mutation simply has to go. Clojure meets its goals by: embracing an industry-standard, open platform - the JVM; modernizing a venerable language - Lisp; fostering functional programming with immutable persistent data structures; and providing built-in concurrency support via software transactional memory and asynchronous agents. The result is robust, practical, and fast. This talk will focus on the motivations, mechanisms and experiences of the implementation of Clojure.},
  isbn = {978-1-60558-270-2},
  series = {{{DLS}} '08}
}

@article{hoosProgrammingOptimization2012,
  title = {Programming by Optimization},
  author = {Hoos, Holger H.},
  year = {2012},
  month = feb,
  volume = {55},
  pages = {70--80},
  issn = {0001-0782},
  doi = {10.1145/2076450.2076469},
  abstract = {Avoid premature commitment, seek design alternatives, and automatically generate performance-optimized software.},
  journal = {Communications of the ACM},
  number = {2}
}

@article{hudakBuildingDomainspecificEmbedded1996,
  title = {Building {{Domain}}-Specific {{Embedded Languages}}},
  author = {Hudak, Paul},
  year = {1996},
  month = dec,
  volume = {28},
  issn = {0360-0300},
  doi = {10.1145/242224.242477},
  journal = {ACM Comput. Surv.},
  number = {4es}
}

@article{jimEfficientEarleyParsing2010,
  title = {Efficient {{Earley Parsing}} with {{Regular Right}}-Hand {{Sides}}},
  author = {Jim, Trevor and Mandelbaum, Yitzhak},
  year = {2010},
  month = sep,
  volume = {253},
  pages = {135--148},
  issn = {1571-0661},
  doi = {10.1016/j.entcs.2010.08.037},
  abstract = {We present a new variant of the Earley parsing algorithm capable of efficiently supporting context-free grammars with regular right hand-sides. We present the core state-machine driven algorithm, the translation of grammars into state machines, and the reconstruction algorithm. We also include a theoretical framework for presenting the algorithm and for evaluating optimizations. Finally, we evaluate the algorithm by testing its implementation.},
  journal = {Electronic Notes in Theoretical Computer Science},
  keywords = {augmented transition networks,Context-free grammars,Earley parsing,regular right sides,scannerless parsing,transducers},
  language = {en},
  number = {7},
  series = {Proceedings of the {{Ninth Workshop}} on {{Language Descriptions Tools}} and {{Applications}} ({{LDTA}} 2009)}
}

@inproceedings{jimNewMethodDependent2011,
  title = {A {{New Method}} for {{Dependent Parsing}}},
  booktitle = {Programming {{Languages}} and {{Systems}}},
  author = {Jim, Trevor and Mandelbaum, Yitzhak},
  editor = {Barthe, Gilles},
  year = {2011},
  pages = {378--397},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-19718-5_20},
  abstract = {Dependent grammars extend context-free grammars by allowing semantic values to be bound to variables and used to constrain parsing. Dependent grammars can cleanly specify common features that cannot be handled by context-free grammars, such as length fields in data formats and significant indentation in programming languages. Few parser generators support dependent parsing, however. To address this shortcoming, we have developed a new method for implementing dependent parsers by extending existing parsing algorithms. Our method proposes a point-free language of dependent grammars, which we believe closely corresponds to existing context-free parsing algorithms, and gives a novel transformation from conventional dependent grammars to point-free ones.To validate our technique, we have specified the semantics of both source and target dependent grammar languages, and proven our transformation sound and complete with respect to those semantics. Furthermore, we have empirically validated the suitability of our point-free language by adapting four parsing engines to support it: an Earley parsing engine; a GLR parsing engine; memoizing, arrow-style parser combinators; and PEG parser combinators.},
  isbn = {978-3-642-19718-5},
  keywords = {Attribute Grammar,Empty String,Lambda Calculus,Semantic Action,Target Language},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{kaminskiModularWellDefinednessAnalysis2013,
  title = {Modular {{Well}}-{{Definedness Analysis}} for {{Attribute Grammars}}},
  booktitle = {Software {{Language Engineering}}},
  author = {Kaminski, Ted and Van Wyk, Eric},
  editor = {Czarnecki, Krzysztof and Hedin, G{\"o}rel},
  year = {2013},
  pages = {352--371},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {We present a modular well-definedness analysis for attribute grammars. The global properties of completeness and non-circularity are ensured with checks on grammar modules that require only additional information from their dependencies. Local checks to ensure global properties are crucial for specifying extensible languages. They allow independent developers of language extensions to verify that their extension, when combined with other independently developed and similarly verified extensions to a specified host language, will result in a composed grammar that is well-defined. Thus, the composition of the host language and user-selected extensions can safely be performed by someone with no expertise in language design and implementation. The analysis is necessarily conservative and imposes some restrictions on the grammar. We argue that the analysis is practical and the restrictions are natural and not burdensome by applying it to the Silver specifications of Silver, our boot-strapped extensible attribute grammar system.},
  isbn = {978-3-642-36089-3},
  keywords = {Attribute Equation,Attribute Grammar,Concrete Syntax,Language Extension,Modular Analysis},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{kaminskiReliableAutomaticComposition2017,
  title = {Reliable and {{Automatic Composition}} of {{Language Extensions}} to {{C}}: {{The ableC Extensible Language Framework}}},
  shorttitle = {Reliable and {{Automatic Composition}} of {{Language Extensions}} to {{C}}},
  author = {Kaminski, Ted and Kramer, Lucas and Carlson, Travis and Van Wyk, Eric},
  year = {2017},
  month = oct,
  volume = {1},
  pages = {98:1--98:29},
  issn = {2475-1421},
  doi = {10.1145/3138224},
  abstract = {This paper describes an extensible language framework, ableC, that allows programmers to import new, domain-specific, independently-developed language features into their programming language, in this case C. Most importantly, this framework ensures that the language extensions will automatically compose to form a working translator that does not terminate abnormally. This is possible due to two modular analyses that extension developers can apply to their language extension to check its composability. Specifically, these ensure that the composed concrete syntax specification is non-ambiguous and the composed attribute grammar specifying the semantics is well-defined. This assurance and the expressiveness of the supported extensions is a distinguishing characteristic of the approach.   The paper describes a number of techniques for specifying a host language, in this case C at the C11 standard, to make it more amenable to language extension. These include techniques that make additional extensions pass these modular analyses, refactorings of the host language to support a wider range of extensions, and the addition of semantic extension points to support, for example, operator overloading and non-local code transformations.},
  journal = {Proc. ACM Program. Lang.},
  keywords = {attribute grammars,context-aware scanning,domain specific languages,extensible compiler frameworks,language composition},
  number = {OOPSLA}
}

@article{kaminskiReliablyComposableLanguage2017,
  title = {Reliably Composable Language Extensions},
  author = {Kaminski, Ted},
  year = {2017},
  month = may,
  doi = {https://doi.org/10.24926/2017.188954},
  abstract = {Many programming tasks are dramatically simpler when an appropriate domain-specific language can be used to accomplish them. These languages offer a variety of potential advantages, including programming at a higher level of abstraction, custom analyses specific to the problem domain, and the ability to generate very efficient code. But they also suffer many disadvantages as a result of their implementation techniques. Fully separate languages (such as YACC, or SQL) are quite flexible, but these are distinct monolithic entities and thus we are unable to draw on the features of several in combination to accomplish a single task. That is, we cannot compose their domain-specific features. "Embedded" DSLs (such as parsing combinators) accomplish something like a different language, but are actually implemented simply as libraries within a flexible host language. This approach allows different libraries to be imported and used together, enabling composition, but it is limited in analysis and translation capabilities by the host language they are embedded within. A promising combination of these two approaches is to allow a host language to be directly extended with new features (syntactic and semantic.) However, while there are plausible ways to attempt to compose language extensions, they can easily fail, making this approach unreliable. Previous methods of assuring reliable composition impose onerous restrictions, such as throwing out entirely the ability to introduce new analysis. This thesis introduces reliably composable language extensions as a technique for the implementation of DSLs. This technique preserves most of the advantages of both separate and "embedded" DSLs. Unlike many prior approaches to language extension, this technique ensures composition of multiple language extensions will succeed, and preserves strong properties about the behavior of the resulting composed compiler. We define an analysis on language extensions that guarantees the composition of several extensions will be well-defined, and we further define a set of testable properties that ensure the resulting compiler will behave as expected, along with a principle that assigns "blame" for bugs that may ultimately appear as a result of composition. Finally, to concretely compare our approach to our original goals for reliably composable language extension, we use these techniques to develop an extensible C compiler front-end, together with several example composable language extensions.},
  language = {en}
}

@inproceedings{karachaliasGADTsMeetTheir2015,
  title = {{{GADTs}} Meet Their Match: Pattern-Matching Warnings That Account for {{GADTs}}, Guards, and Laziness},
  shorttitle = {{{GADTs}} Meet Their Match},
  booktitle = {Proceedings of the 20th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Karachalias, Georgios and Schrijvers, Tom and Vytiniotis, Dimitrios and Jones, Simon Peyton},
  year = {2015},
  month = aug,
  pages = {424--436},
  publisher = {{Association for Computing Machinery}},
  address = {{Vancouver, BC, Canada}},
  doi = {10.1145/2784731.2784748},
  abstract = {For ML and Haskell, accurate warnings when a function definition has redundant or missing patterns are mission critical. But today's compilers generate bogus warnings when the programmer uses guards (even simple ones), GADTs, pattern guards, or view patterns. We give the first algorithm that handles all these cases in a single, uniform framework, together with an implementation in GHC, and evidence of its utility in practice.},
  isbn = {978-1-4503-3669-7},
  keywords = {Generalized Algebraic Data Types,Haskell,OutsideIn(X),pattern matching},
  series = {{{ICFP}} 2015}
}

@inproceedings{katsPureDeclarativeSyntax2010,
  title = {Pure and {{Declarative Syntax Definition}}: {{Paradise Lost}} and {{Regained}}},
  shorttitle = {Pure and {{Declarative Syntax Definition}}},
  booktitle = {Proceedings of the {{ACM International Conference}} on {{Object Oriented Programming Systems Languages}} and {{Applications}}},
  author = {Kats, Lennart C.L. and Visser, Eelco and Wachsmuth, Guido},
  year = {2010},
  pages = {918--932},
  publisher = {{ACM}},
  address = {{Reno/Tahoe, Nevada, USA}},
  doi = {10.1145/1869459.1869535},
  abstract = {Syntax definitions are pervasive in modern software systems, and serve as the basis for language processing tools like parsers and compilers. Mainstream parser generators pose restrictions on syntax definitions that follow from their implementation algorithm. They hamper evolution, maintainability, and compositionality of syntax definitions. The pureness and declarativity of syntax definitions is lost. We analyze how these problems arise for different aspects of syntax definitions, discuss their consequences for language engineers, and show how the pure and declarative nature of syntax definitions can be regained.},
  isbn = {978-1-4503-0203-6},
  keywords = {declarative,grammars,grammarware,parsers,sdf,sglr,syntax definition},
  series = {{{OOPSLA}} '10}
}

@inproceedings{katsSpoofaxLanguageWorkbench2010,
  title = {The {{Spoofax Language Workbench}}: {{Rules}} for {{Declarative Specification}} of {{Languages}} and {{IDEs}}},
  shorttitle = {The {{Spoofax Language Workbench}}},
  booktitle = {Proceedings of the {{ACM International Conference}} on {{Object Oriented Programming Systems Languages}} and {{Applications}}},
  author = {Kats, Lennart C.L. and Visser, Eelco},
  year = {2010},
  pages = {444--463},
  publisher = {{ACM}},
  address = {{Reno/Tahoe, Nevada, USA}},
  doi = {10.1145/1869459.1869497},
  abstract = {Spoofax is a language workbench for efficient, agile development of textual domain-specific languages with state-of-the-art IDE support. Spoofax integrates language processing techniques for parser generation, meta-programming, and IDE development into a single environment. It uses concise, declarative specifications for languages and IDE services. In this paper we describe the architecture of Spoofax and introduce idioms for high-level specifications of language semantics using rewrite rules, showing how analyses can be reused for transformations, code generation, and editor services such as error marking, reference resolving, and content completion. The implementation of these services is supported by language-parametric editor service classes that can be dynamically loaded by the Eclipse IDE, allowing new languages to be developed and used side-by-side in the same Eclipse environment.},
  isbn = {978-1-4503-0203-6},
  keywords = {domain-specific language,dsl,eclipse,IDE,language workbench,meta-tooling,sdf,sglr,spoofax,stratego},
  series = {{{OOPSLA}} '10}
}

@incollection{kitchinOrc2009,
  title = {The {{Orc}} Programming Language},
  booktitle = {Formal Techniques for {{Distributed Systems}}},
  author = {Kitchin, David and Quark, Adrian and Cook, William and Misra, Jayadev},
  year = {2009},
  pages = {1--25},
  publisher = {{Springer}}
}

@inproceedings{klintRASCALDomainSpecific2009,
  title = {{{RASCAL}}: {{A Domain Specific Language}} for {{Source Code Analysis}} and {{Manipulation}}},
  shorttitle = {{{RASCAL}}},
  booktitle = {2009 {{Ninth IEEE International Working Conference}} on {{Source Code Analysis}} and {{Manipulation}}},
  author = {Klint, Paul and van der Storm, Tijs and Vinju, Jurgen},
  year = {2009},
  month = sep,
  pages = {168--177},
  issn = {null},
  doi = {10.1109/SCAM.2009.28},
  abstract = {Many automated software engineering tools require tight integration of techniques for source code analysis and manipulation. State-of-the-art tools exist for both, but the domains have remained notoriously separate because different computational paradigms fit each domain best. This impedance mismatch hampers the development of new solutions because the desired functionality and scalability can only be achieved by repeated and ad hoc integration of different techniques. RASCAL is a domain-specific language that takes away most of this boilerplate by integrating source code analysis and manipulation at the conceptual, syntactic, semantic and technical level. We give an overview of the language and assess its merits by implementing a complex refactoring.},
  keywords = {ad hoc integration,automated software engineering tool,complex software refactoring,conceptual-syntactic-semantic-technical level,domain specific language,Domain specific languages,Impedance,impedance mismatch,Informatics,Java,Libraries,Logic programming,meta-programming,object-oriented languages,Pattern matching,program diagnostics,RASCAL,Scalability,Software engineering,software maintenance,source code analysis,source code manipulation,Storms,transformation}
}

@inproceedings{langDeterministicTechniquesEfficient1974,
  ids = {langDeterministicTechniquesEfficient1974a},
  title = {Deterministic {{Techniques}} for {{Efficient Non}}-{{Deterministic Parsers}}},
  booktitle = {Automata, {{Languages}} and {{Programming}}},
  author = {Lang, Bernard},
  editor = {Loeckx, Jacques},
  year = {1974},
  pages = {255--269},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-662-21545-6_18},
  abstract = {A general study of parallel non-deterministic parsing and translation {\`a} la Earley is developped formally, based on non-deterministic pushdown acceptor-transducers. Several results (camplexity and efficiency) are established, same new and other previously proved only in special cases. As an application, we show that for every family of deterministic context-free pushdown parsers (e.g. precedence, LR(k), LL(k), ...) there is a family of general context-free parallel parsers that have the same efficiency in most practical cases (e.g. analysis of programming languages).},
  isbn = {978-3-662-21545-6},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{leoGeneralContextfreeParsing1991,
  title = {A General Context-Free Parsing Algorithm Running in Linear Time on Every {{LR}}(k) Grammar without Using Lookahead},
  author = {Leo, Joop M. I. M.},
  year = {1991},
  month = may,
  volume = {82},
  pages = {165--176},
  issn = {0304-3975},
  doi = {10.1016/0304-3975(91)90180-A},
  abstract = {A new general context-free parsing algorithm is presented which runs in linear time and space on every LR(k) grammar without using any lookahead and without making use of the LR property. Most of the existing implementations of tabular parsing algorithms, including those using lookahead, can easily be adapted to this new algorithm without a noteworthy loss of efficiency. For some natural right recursive grammars both the time and space complexity will be improved from {$\Omega$}(n2) to O(n). This makes this algorithm not only of theoretical but probably of practical interest as well.},
  journal = {Theoretical Computer Science},
  number = {1}
}

@techreport{leroyOCamlSystemRelease2018,
  title = {The {{OCaml}} System Release 4.07: {{Documentation}} and User's Manual},
  shorttitle = {The {{OCaml}} System Release 4.07},
  author = {Leroy, Xavier and Doligez, Damien and Frisch, Alain and Garrigue, Jacques and R{\'e}my, Didier and Vouillon, J{\'e}r{\^o}me},
  year = {2018},
  month = jul,
  abstract = {This manual documents the release 4.07 of the OCaml system. It is organized as follows. Part I, "An introduction to OCaml", gives an overview of the language. Part II, "The OCaml language", is the reference description of the language. Part III, "The OCaml tools", documents the compilers, toplevel system, and programming utilities. Part IV, "The OCaml library", describes the modules provided in the standard library.},
  language = {en},
  type = {Report}
}

@inproceedings{lorchArmadaLoweffortVerification2020,
  title = {Armada: Low-Effort Verification of High-Performance Concurrent Programs},
  shorttitle = {Armada},
  booktitle = {Proceedings of the 41st {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Lorch, Jacob R. and Chen, Yixuan and Kapritsos, Manos and Parno, Bryan and Qadeer, Shaz and Sharma, Upamanyu and Wilcox, James R. and Zhao, Xueyuan},
  year = {2020},
  month = jun,
  pages = {197--210},
  publisher = {{Association for Computing Machinery}},
  address = {{London, UK}},
  doi = {10.1145/3385412.3385971},
  abstract = {Safely writing high-performance concurrent programs is notoriously difficult. To aid developers, we introduce Armada, a language and tool designed to formally verify such programs with relatively little effort. Via a C-like language and a small-step, state-machine-based semantics, Armada gives developers the flexibility to choose arbitrary memory layout and synchronization primitives so they are never constrained in their pursuit of performance. To reduce developer effort, Armada leverages SMT-powered automation and a library of powerful reasoning techniques, including rely-guarantee, TSO elimination, reduction, and alias analysis. All these techniques are proven sound, and Armada can be soundly extended with additional strategies over time. Using Armada, we verify four concurrent case studies and show that we can achieve performance equivalent to that of unverified code.},
  isbn = {978-1-4503-7613-6},
  keywords = {refinement,weak memory models,x86-TSO},
  series = {{{PLDI}} 2020}
}

@inproceedings{lorenzenSoundTypedependentSyntactic2016,
  title = {Sound {{Type}}-Dependent {{Syntactic Language Extension}}},
  booktitle = {Proceedings of the 43rd {{Annual ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Lorenzen, Florian and Erdweg, Sebastian},
  year = {2016},
  pages = {204--216},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2837614.2837644},
  abstract = {Syntactic language extensions can introduce new facilities into a programming language while requiring little implementation effort and modest changes to the compiler. It is typical to desugar language extensions in a distinguished compiler phase after parsing or type checking, not affecting any of the later compiler phases. If desugaring happens before type checking, the desugaring cannot depend on typing information and type errors are reported in terms of the generated code. If desugaring happens after type checking, the code generated by the desugaring is not type checked and may introduce vulnerabilities. Both options are undesirable. We propose a system for syntactic extensibility where desugaring happens after type checking and desugarings are guaranteed to only generate well-typed code. A major novelty of our work is that desugarings operate on typing derivations instead of plain syntax trees. This provides desugarings access to typing information and forms the basis for the soundness guarantee we provide, namely that a desugaring generates a valid typing derivation. We have implemented our system for syntactic extensibility in a language-independent fashion and instantiated it for a substantial subset of Java, including generics and inheritance. We provide a sound Java extension for Scala-like for-comprehensions.},
  isbn = {978-1-4503-3549-2},
  keywords = {automatic verification,Language extensibility,macros,metaprogramming,type soundness,type-dependent desugaring},
  series = {{{POPL}} '16}
}

@article{marangetWarningsPatternMatching2007,
  title = {Warnings for Pattern Matching},
  author = {Maranget, Luc},
  year = {2007},
  month = may,
  volume = {17},
  pages = {387--421},
  publisher = {{Cambridge University Press}},
  issn = {1469-7653, 0956-7968},
  doi = {10.1017/S0956796807006223},
  abstract = {We examine the ML pattern-matching anomalies of useless clauses and non-exhaustive matches. We state the definition of these anomalies, building upon pattern matching semantics, and propose a simple algorithm to detect them. We have integrated the algorithm in the Objective Caml compiler, but we show that the same algorithm is also usable in a non-strict language such as Haskell. Or-patterns are considered for both strict and non-strict languages.},
  journal = {Journal of Functional Programming},
  language = {en},
  number = {3}
}

@inproceedings{matsakisRustLanguage2014,
  title = {The {{Rust Language}}},
  booktitle = {Proceedings of the 2014 {{ACM SIGAda Annual Conference}} on {{High Integrity Language Technology}}},
  author = {Matsakis, Nicholas D. and Klock, II, Felix S.},
  year = {2014},
  pages = {103--104},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2663171.2663188},
  abstract = {Rust is a new programming language for developing reliable and efficient systems. It is designed to support concurrency and parallelism in building applications and libraries that take full advantage of modern hardware. Rust's static type system is safe and expressive and provides strong guarantees about isolation, concurrency, and memory safety.

Rust also offers a clear performance model, making it easier to predict and reason about program efficiency. One important way it accomplishes this is by allowing fine-grained control over memory representations, with direct support for stack allocation and contiguous record storage. The language balances such controls with the absolute requirement for safety: Rust's type system and runtime guarantee the absence of data races, buffer overflows, stack overflows, and accesses to uninitialized or deallocated memory.},
  isbn = {978-1-4503-3217-0},
  keywords = {affine type systems,memory management,rust,systems programming},
  series = {{{HILT}} '14}
}

@inproceedings{morrisExploringRegularTree2006,
  title = {Exploring the {{Regular Tree Types}}},
  booktitle = {Types for {{Proofs}} and {{Programs}}},
  author = {Morris, Peter and Altenkirch, Thorsten and McBride, Conor},
  editor = {Filli{\^a}tre, Jean-Christophe and {Paulin-Mohring}, Christine and Werner, Benjamin},
  year = {2006},
  pages = {252--267},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/11617990_16},
  abstract = {In this paper we use the Epigram language to define the universe of regular tree types\textemdash{}closed under empty, unit, sum, product and least fixpoint. We then present a generic decision procedure for Epigram's in-built equality at each type, taking a complementary approach to that of Benke, Dybjer and Jansson [7]. We also give a generic definition of map, taking our inspiration from Jansson and Jeuring [21]. Finally, we equip the regular universe with the partial derivative which can be interpreted functionally as Huet's notion of `zipper', as suggested by McBride in [27] and implemented (without the fixpoint case) in Generic Haskell by Hinze, Jeuring and L{\"o}h [18]. We aim to show through these examples that generic programming can be ordinary programming in a dependently typed language.},
  isbn = {978-3-540-31429-5},
  keywords = {Functional Programming,Functional Programming Language,International Summer School,Type Constructor,Type Theory},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{neronTheoryNameResolution2015,
  title = {A {{Theory}} of {{Name Resolution}}},
  booktitle = {Programming {{Languages}} and {{Systems}}},
  author = {Neron, Pierre and Tolmach, Andrew and Visser, Eelco and Wachsmuth, Guido},
  editor = {Vitek, Jan},
  year = {2015},
  pages = {205--231},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {We describe a language-independent theory for name binding and resolution, suitable for programming languages with complex scoping rules including both lexical scoping and modules. We formulate name resolution as a two-stage problem. First a language-independent scope graph is constructed using language-specific rules from an abstract syntax tree. Then references in the scope graph are resolved to corresponding declarations using a language-independent resolution process. We introduce a resolution calculus as a concise, declarative, and languageindependent specification of name resolution. We develop a resolution algorithm that is sound and complete with respect to the calculus. Based on the resolution calculus we develop language-independent definitions of {$\alpha$}-equivalence and rename refactoring. We illustrate the approach using a small example language with modules. In addition, we show how our approach provides a model for a range of name binding patterns in existing languages.},
  isbn = {978-3-662-46669-8},
  keywords = {Abstract Syntax Tree,Binding Pattern,Code Completion,Resolution Algorithm,Visibility Policy},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{oderskySimplicitlyFoundationsApplications2017,
  title = {Simplicitly: {{Foundations}} and {{Applications}} of {{Implicit Function Types}}},
  shorttitle = {Simplicitly},
  author = {Odersky, Martin and Blanvillain, Olivier and Liu, Fengyun and Biboudis, Aggelos and Miller, Heather and Stucki, Sandro},
  year = {2017},
  month = dec,
  volume = {2},
  pages = {42:1--42:29},
  issn = {2475-1421},
  doi = {10.1145/3158130},
  abstract = {Understanding a program entails understanding its context; dependencies, configurations and even implementations are all forms of contexts. Modern programming languages and theorem provers offer an array of constructs to define contexts, implicitly. Scala offers implicit parameters which are used pervasively, but which cannot be abstracted over.  This paper describes a generalization of implicit parameters to implicit function types, a powerful way to abstract over the context in which some piece of code is run. We provide a formalization based on bidirectional type-checking that closely follows the semantics implemented by the Scala compiler.  To demonstrate their range of abstraction capabilities, we present several applications that make use of implicit function types. We show how to encode the builder pattern, tagless interpreters, reader and free monads and we assess the performance of the monadic structures presented.},
  journal = {Proc. ACM Program. Lang.},
  keywords = {Dotty,implicit parameters,Scala},
  number = {POPL}
}

@inproceedings{omarSafelyComposableTypeSpecific2014,
  title = {Safely {{Composable Type}}-{{Specific Languages}}},
  booktitle = {{{ECOOP}} 2014 \textendash{} {{Object}}-{{Oriented Programming}}},
  author = {Omar, Cyrus and Kurilova, Darya and Nistor, Ligia and Chung, Benjamin and Potanin, Alex and Aldrich, Jonathan},
  editor = {Jones, Richard},
  year = {2014},
  pages = {105--130},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {Programming languages often include specialized syntax for common datatypes (e.g. lists) and some also build in support for specific specialized datatypes (e.g. regular expressions), but user-defined types must use general-purpose syntax. Frustration with this causes developers to use strings, rather than structured data, with alarming frequency, leading to correctness, performance, security, and usability issues. Allowing library providers to modularly extend a language with new syntax could help address these issues. Unfortunately, prior mechanisms either limit expressiveness or are not safely composable: individually unambiguous extensions can still cause ambiguities when used together. We introduce type-specific languages (TSLs): logic associated with a type that determines how the bodies of generic literals, able to contain arbitrary syntax, are parsed and elaborated, hygienically. The TSL for a type is invoked only when a literal appears where a term of that type is expected, guaranteeing non-interference. We give evidence supporting the applicability of this approach and formally specify it with a bidirectionally typed elaboration semantics for the Wyvern programming language.},
  isbn = {978-3-662-44202-9},
  keywords = {bidirectional typechecking,extensible languages,hygiene,parsing},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@book{palmkvistBuildingProgrammingLanguages2018,
  title = {Building {{Programming Languages}}, {{Construction}} by {{Construction}}},
  author = {Palmkvist, Viktor},
  year = {2018},
  publisher = {{KTH, School of Electrical Engineering and Computer Science (EECS)}},
  abstract = {The task of implementing a programming language is a task that entails a great deal of work. Yet much of this work is similar for different programming languages: most languages require, e.g., parsing, name resolution, type-checking, and optimization. When implementing domain-specific languages (DSLs) the reimplementation of these largely similar tasks seems especially redundant. A number of approaches exist to alleviate this issue, including embedded DSLs, macro-rewriting systems, and more general systems intended for language implementation. However, these tend to have at least one of the following limitations: They present a leaky abstraction, e.g., error messages do not refer to the DSL but rather some other programming language, namely the one used to implement the DSL. They limit the flexibility of the DSL, either to the constructs present in another language, or merely to the syntax of some other language. They see an entire language as the unit of composition. Complete languages are extended with other complete language extensions. Instead, this thesis introduces the concept of a syntax construction, which represents a smaller unit of composition. A syntax construction defines a single language feature, e.g., an if-statement, an anonymous function, or addition. Each syntax construction specifies its own syntax, binding semantics, and runtime semantics, independent of the rest of the language. The runtime semantics are defined using a translation into another target language, similarly to macros. These translations can then be checked to ensure that they preserve binding semantics and introduce no binding errors. This checking ensures that binding errors can be presented in terms of code the programmer wrote, rather than generated code in some underlying language. During evaluation several limitations are encountered. Removing or minimizing these limitations appears possible, but is left for future work},
  keywords = {domain-specific language,programming language construction},
  language = {eng},
  number = {2018:408},
  series = {{{TRITA}}-{{EECS}}-{{EX}}}
}

@inproceedings{palmkvistCreatingDomainSpecificLanguages2019,
  title = {Creating {{Domain}}-{{Specific Languages}} by {{Composing Syntactical Constructs}}},
  booktitle = {Practical {{Aspects}} of {{Declarative Languages}}},
  author = {Palmkvist, Viktor and Broman, David},
  editor = {Alferes, Jos{\'e} J{\'u}lio and Johansson, Moa},
  year = {2019},
  pages = {187--203},
  publisher = {{Springer International Publishing}},
  abstract = {Creating a programming language is a considerable undertaking, even for relatively small domain-specific languages (DSLs). Most approaches to ease this task either limit the flexibility of the DSL or consider entire languages as the unit of composition. This paper presents a new approach using syntactical constructs (also called syncons) for defining DSLs in much smaller units of composition while retaining flexibility. A syntactical construct defines a single language feature, such as an if statement or an anonymous function. Each syntactical construct is fully self-contained: it specifies its own concrete syntax, binding semantics, and runtime semantics, independently of the rest of the language. The runtime semantics are specified as a translation to a user defined target language, while the binding semantics allow name resolution before expansion. Additionally, we present a novel approach for dealing with syntactical ambiguity that arises when combining languages, even if the languages are individually unambiguous. The work is implemented and evaluated in a case study, where small subsets of OCaml and Lua have been defined and composed using syntactical constructs.},
  isbn = {978-3-030-05998-9},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{parrAdaptiveLLParsing2014,
  title = {Adaptive {{LL}}(*) {{Parsing}}: {{The Power}} of {{Dynamic Analysis}}},
  shorttitle = {Adaptive {{LL}}(*) {{Parsing}}},
  booktitle = {Proceedings of the 2014 {{ACM International Conference}} on {{Object Oriented Programming Systems Languages}} \& {{Applications}}},
  author = {Parr, Terence and Harwell, Sam and Fisher, Kathleen},
  year = {2014},
  pages = {579--598},
  publisher = {{ACM}},
  address = {{Portland, Oregon, USA}},
  doi = {10.1145/2660193.2660202},
  abstract = {Despite the advances made by modern parsing strategies such as PEG, LL(*), GLR, and GLL, parsing is not a solved problem. Existing approaches suffer from a number of weaknesses, including difficulties supporting side-effecting embedded actions, slow and/or unpredictable performance, and counter-intuitive matching strategies. This paper introduces the ALL(*) parsing strategy that combines the simplicity, efficiency, and predictability of conventional top-down LL(k) parsers with the power of a GLR-like mechanism to make parsing decisions. The critical innovation is to move grammar analysis to parse-time, which lets ALL(*) handle any non-left-recursive context-free grammar. ALL(*) is O(n4) in theory but consistently performs linearly on grammars used in practice, outperforming general strategies such as GLL and GLR by orders of magnitude. ANTLR 4 generates ALL(*) parsers and supports direct left-recursion through grammar rewriting. Widespread ANTLR 4 use (5000 downloads/month in 2013) provides evidence that ALL(*) is effective for a wide variety of applications.},
  isbn = {978-1-4503-2585-1},
  keywords = {all(*),augmented transition networks,dfa,gll,glr,grammar,ll(*),nondeterministic parsing,peg},
  series = {{{OOPSLA}} '14}
}

@inproceedings{parrLLFoundationANTLR2011,
  title = {{{LL}}(*): {{The Foundation}} of the {{ANTLR Parser Generator}}},
  shorttitle = {{{LL}}(*)},
  booktitle = {Proceedings of the {{32Nd ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Parr, Terence and Fisher, Kathleen},
  year = {2011},
  pages = {425--436},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1993498.1993548},
  abstract = {Despite the power of Parser Expression Grammars (PEGs) and GLR, parsing is not a solved problem. Adding nondeterminism (parser speculation) to traditional LL and LR parsers can lead to unexpected parse-time behavior and introduces practical issues with error handling, single-step debugging, and side-effecting embedded grammar actions. This paper introduces the LL(*) parsing strategy and an associated grammar analysis algorithm that constructs LL(*) parsing decisions from ANTLR grammars. At parse-time, decisions gracefully throttle up from conventional fixed k{$>$}=1 lookahead to arbitrary lookahead and, finally, fail over to backtracking depending on the complexity of the parsing decision and the input symbols. LL(*) parsing strength reaches into the context-sensitive languages, in some cases beyond what GLR and PEGs can express. By statically removing as much speculation as possible, LL(*) provides the expressivity of PEGs while retaining LL's good error handling and unrestricted grammar actions. Widespread use of ANTLR (over 70,000 downloads/year) shows that it is effective for a wide variety of applications.},
  isbn = {978-1-4503-0663-8},
  keywords = {augmented transition networks,backtracking,context-sensitive parsing,deterministic finite automata,glr,memoization,nondeterministic parsing,peg,semantic predicates,subset construction,syntactic predicates},
  series = {{{PLDI}} '11}
}

@article{rademakerRewritingSemanticsSoftware2005,
  title = {A {{Rewriting Semantics}} for a {{Software Architecture Description Language}}},
  author = {Rademaker, Alexandre and Braga, Christiano and Sztajnberg, Alexandre},
  year = {2005},
  month = may,
  volume = {130},
  pages = {345--377},
  issn = {1571-0661},
  doi = {10.1016/j.entcs.2005.03.018},
  abstract = {Distributed and concurrent application invariably have coordination requirements. The design of those applications, composed by several (possibly distributed) components, has to consider coordination requirements comprising inter-component interaction styles, and intra-component concurrency and synchronization aspects. In our approach coordination aspects are treated in the software architecture level and can be specified in high-level contracts in CBabel ADL. A rewriting logic semantics for the software architecture description language CBabel is given, revisiting and extending previous work by some of the authors, which now includes a revision of the previous semantics and the addition of new features covering all the language. The CBabel tool is also presented. The CBabel tool is a prototype executable environment for CBabel, that implements the given CBabel's rewriting logic semantics and allows the execution and verification of CBabel descriptions in the Maude system, an implementation of rewriting logic. In this way, software architectures describing complex applications can be formally verified regarding properties such as deadlock and synchronization consistency in the software architecture design phase of its life cycle.},
  journal = {Electronic Notes in Theoretical Computer Science},
  keywords = {contracts,Maude,rewriting logic,software architecture description languages},
  language = {en},
  series = {Proceedings of the {{Brazilian Symposium}} on {{Formal Methods}} ({{SBMF}} 2004)}
}

@inproceedings{rompfLightweightModularStaging2010,
  title = {Lightweight {{Modular Staging}}: {{A Pragmatic Approach}} to {{Runtime Code Generation}} and {{Compiled DSLs}}},
  shorttitle = {Lightweight {{Modular Staging}}},
  booktitle = {Proceedings of the {{Ninth International Conference}} on {{Generative Programming}} and {{Component Engineering}}},
  author = {Rompf, Tiark and Odersky, Martin},
  year = {2010},
  pages = {127--136},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1868294.1868314},
  abstract = {Software engineering demands generality and abstraction, performance demands specialization and concretization. Generative programming can provide both, but the effort required to develop high-quality program generators likely offsets their benefits, even if a multi-stage programming language is used. We present lightweight modular staging, a library-based multi-stage programming approach that breaks with the tradition of syntactic quasi-quotation and instead uses only types to distinguish between binding times. Through extensive use of component technology, lightweight modular staging makes an optimizing compiler framework available at the library level, allowing programmers to tightly integrate domain-specific abstractions and optimizations into the generation process. We argue that lightweight modular staging enables a form of language virtualization, i.e. allows to go from a pure-library embedded language to one that is practically equivalent to a stand-alone implementation with only modest effort.},
  isbn = {978-1-4503-0154-1},
  keywords = {code generation,domain-specific languages,language virtualization,multi-stage programming},
  series = {{{GPCE}} '10}
}

@article{rompfLightweightModularStaging2012,
  title = {Lightweight {{Modular Staging}}: {{A Pragmatic Approach}} to {{Runtime Code Generation}} and {{Compiled DSLs}}},
  shorttitle = {Lightweight {{Modular Staging}}},
  author = {Rompf, Tiark and Odersky, Martin},
  year = {2012},
  month = jun,
  volume = {55},
  pages = {121--130},
  issn = {0001-0782},
  doi = {10.1145/2184319.2184345},
  abstract = {Good software engineering practice demands generalization and abstraction, whereas high performance demands specialization and concretization. These goals are at odds, and compilers can only rarely translate expressive high-level programs to modern hardware platforms in a way that makes best use of the available resources. Generative programming is a promising alternative to fully automatic translation. Instead of writing down the target program directly, developers write a program generator, which produces the target program as its output. The generator can be written in a high-level, generic style and can still produce efficient, specialized target programs. In practice, however, developing high-quality program generators requires a very large effort that is often hard to amortize. We present lightweight modular staging (LMS), a generative programming approach that lowers this effort significantly. LMS seamlessly combines program generator logic with the generated code in a single program, using only types to distinguish the two stages of execution. Through extensive use of component technology, LMS makes a reusable and extensible compiler framework available at the library level, allowing programmers to tightly integrate domain-specific abstractions and optimizations into the generation process, with common generic optimizations provided by the framework. LMS is well suited to develop embedded domain-specific languages (DSLs) and has been used to develop powerful performance-oriented DSLs for demanding domains such as machine learning, with code generation for heterogeneous platforms including GPUs. LMS has also been used to generate SQL for embedded database queries and JavaScript for web applications.},
  journal = {Commun. ACM},
  number = {6}
}

@inproceedings{rompfReflectionsLMSExploring2016,
  title = {Reflections on {{LMS}}: {{Exploring Front}}-End {{Alternatives}}},
  shorttitle = {Reflections on {{LMS}}},
  booktitle = {Proceedings of the 2016 7th {{ACM SIGPLAN Symposium}} on {{Scala}}},
  author = {Rompf, Tiark},
  year = {2016},
  pages = {41--50},
  publisher = {{ACM}},
  address = {{Amsterdam, Netherlands}},
  doi = {10.1145/2998392.2998399},
  abstract = {Metaprogramming techniques to generate code at runtime in a general-purpose meta-language have seen a surge of interest in recent years, driven by the widening performance gap between high-level languages and emerging hardware platforms. In the context of Scala, the LMS (Lightweight Modular Staging) framework has contributed to ``abstraction without regret''--high-level programming without performance penalty--in a number of challenging domains, through runtime code generation and embedded compiler pipelines based on stacks of DSLs. Based on this experience, this paper crystallizes some of the design decisions of LMS and discusses potential alternatives, which maintain the underlying spirit but differ in implementation choices: specifically, strategies for realizing more flexible front-end embeddings using type classes instead of higher-kinded types, and strategies for type-safe metaprogramming with untyped intermediate representations.},
  isbn = {978-1-4503-4648-1},
  keywords = {domain-specific languages,intermediate representation,Multi-stage programming},
  series = {{{SCALA}} 2016}
}

@inproceedings{schmitzConservativeAmbiguityDetection2007,
  title = {Conservative {{Ambiguity Detection}} in {{Context}}-{{Free Grammars}}},
  booktitle = {Automata, {{Languages}} and {{Programming}}},
  author = {Schmitz, Sylvain},
  editor = {Arge, Lars and Cachin, Christian and Jurdzi{\'n}ski, Tomasz and Tarlecki, Andrzej},
  year = {2007},
  pages = {692--703},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {The ability to detect ambiguities in context-free grammars is vital for their use in several fields, but the problem is undecidable in the general case. We present a safe, conservative approach, where the approximations cannot result in overlooked ambiguous cases . We analyze the complexity of the verification, and provide formal comparisons with several other ambiguity detection methods.},
  isbn = {978-3-540-73420-8},
  keywords = {Derivation Tree,Nondeterministic Automaton,Nonterminal Symbol,Position Graph,Regular Approximation},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{schwerdfegerVerifiableCompositionDeterministic2009,
  title = {Verifiable {{Composition}} of {{Deterministic Grammars}}},
  booktitle = {Proceedings of the 30th {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Schwerdfeger, August C. and Van Wyk, Eric R.},
  year = {2009},
  pages = {199--210},
  publisher = {{ACM}},
  address = {{Dublin, Ireland}},
  doi = {10.1145/1542476.1542499},
  abstract = {There is an increasing interest in extensible languages, (domain-specific) language extensions, and mechanisms for their specification and implementation. One challenge is to develop tools that allow non-expert programmers to add an eclectic set of language extensions to a host language. We describe mechanisms for composing and analyzing concrete syntax specifications of a host language and extensions to it. These specifications consist of context-free grammars with each terminal symbol mapped to a regular expression, from which a slightly-modified LR parser and context-aware scanner are generated. Traditionally, conflicts are detected when a parser is generated from the composed grammar, but this comes too late since it is the non-expert programmer directing the composition of independently developed extensions with the host language. The primary contribution of this paper is a modular analysis that is performed independently by each extension designer on her extension (composed alone with the host language). If each extension passes this modular analysis, then the language composed later by the programmer will compile with no conflicts or lexical ambiguities. Thus, extension writers can verify that their extension will safely compose with others and, if not, fix the specification so that it will. This is possible due to the context-aware scanner's lexical disambiguation and a set of reasonable restrictions limiting the constructs that can be introduced by an extension. The restrictions ensure that the parse table states can be partitioned so that each state can be attributed to the host language or a single extension.},
  isbn = {978-1-60558-392-1},
  keywords = {context-aware scanning,extensible languages,grammar composition,language composition,lr parsing},
  series = {{{PLDI}} '09}
}

@article{scottGLLParsing2010,
  title = {{{GLL Parsing}}},
  author = {Scott, Elizabeth and Johnstone, Adrian},
  year = {2010},
  month = sep,
  volume = {253},
  pages = {177--189},
  issn = {1571-0661},
  doi = {10.1016/j.entcs.2010.08.041},
  abstract = {Recursive Descent (RD) parsers are popular because their control flow follows the structure of the grammar and hence they are easy to write and to debug. However, the class of grammars which admit RD parsers is very limited. Backtracking techniques may be used to extend this class, but can have explosive runtimes and cannot deal with grammars with left recursion. Tomita-style RNGLR parsers are fully general but are based on LR techniques and do not have the direct relationship with the grammar that an RD parser has. We develop the fully general GLL parsing technique which is recursive descent-like, and has the property that the parse follows closely the structure of the grammar rules, but uses RNGLR-like machinery to handle non-determinism. The resulting recognisers run in worst-case cubic time and can be built even for left recursive grammars.},
  journal = {Electronic Notes in Theoretical Computer Science},
  keywords = {context free languages,generalised parsing,recursive descent,RNGLR and RIGLR parsing},
  number = {7},
  series = {Proceedings of the {{Ninth Workshop}} on {{Language Descriptions Tools}} and {{Applications}} ({{LDTA}} 2009)}
}

@article{scottSPPFStyleParsingEarley2008,
  title = {{{SPPF}}-{{Style Parsing From Earley Recognisers}}},
  author = {Scott, Elizabeth},
  year = {2008},
  month = apr,
  volume = {203},
  pages = {53--67},
  issn = {1571-0661},
  doi = {10.1016/j.entcs.2008.03.044},
  abstract = {In its recogniser form, Earley's algorithm for testing whether a string can be derived from a grammar is worst case cubic on general context free grammars (CFG). Earley gave an outline of a method for turning his recognisers into parsers, but it turns out that this method is incorrect. Tomita's GLR parser returns a shared packed parse forest (SPPF) representation of all derivations of a given string from a given CFG but is worst case unbounded polynomial order. We have given a modified worst-case cubic version, the BRNGLR algorithm, that, for any string and any CFG, returns a binarised SPPF representation of all possible derivations of a given string. In this paper we apply similar techniques to develop two versions of an Earley parsing algorithm that, in worst-case cubic time, return an SPPF representation of all derivations of a given string from a given CFG.},
  journal = {Electronic Notes in Theoretical Computer Science},
  keywords = {context free languages,cubic generalised parsing,Earley parsing},
  language = {en},
  number = {2},
  series = {Proceedings of the {{Seventh Workshop}} on {{Language Descriptions}}, {{Tools}}, and {{Applications}} ({{LDTA}} 2007)}
}

@inproceedings{sestoftMLPatternMatch1996,
  title = {{{ML}} Pattern Match Compilation and Partial Evaluation},
  booktitle = {Partial {{Evaluation}}},
  author = {Sestoft, Peter},
  editor = {Danvy, Olivier and Gl{\"u}ck, Robert and Thiemann, Peter},
  year = {1996},
  pages = {446--464},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/3-540-61580-6_22},
  abstract = {We derive a compiler for ML-style pattern matches. It is conceptually simple and produces reasonably good compiled matches. The derivation is inspired by the instrumentation and partial evaluation of na{\"I}ve string matchers. Following that paradigm, we first present a general and na{\"I}ve ML pattern matcher, instrument it to collect and exploit extra information, and show that partial evaluation of the instrumented general matcher with respect to a given match produces an efficient specialized matcher.We then discard the partial evaluator and show that a match compiler can be obtained just by slightly modifying the instrumented general matcher. The resulting match compiler is interesting in its own right, and naturally detects inexhaustive matches and redundant match rules.},
  isbn = {978-3-540-70589-5},
  keywords = {Match Rule,Negative Information,Partial Evaluation,Pattern Match,Term Description},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{sheardTemplateMetaprogrammingHaskell2002,
  title = {Template {{Meta}}-Programming for {{Haskell}}},
  booktitle = {Proceedings of the 2002 {{ACM SIGPLAN Workshop}} on {{Haskell}}},
  author = {Sheard, Tim and Jones, Simon Peyton},
  year = {2002},
  pages = {1--16},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/581690.581691},
  abstract = {We propose a new extension to the purely functional programming language Haskell that supports compile-time meta-programming. The purpose of the system is to support the algorithmic construction of programs at compile-time.The ability to generate code at compile time allows the programmer to implement such features as polytypic programs, macro-like expansion, user directed optimization (such as inlining), and the generation of supporting data structures and functions from existing data structures and functions.Our design is being implemented in the Glasgow Haskell Compiler, ghc.},
  isbn = {978-1-58113-605-0},
  keywords = {meta programming,templates},
  series = {Haskell '02}
}

@inproceedings{silkensenWellTypedIslandsParse2013,
  title = {Well-{{Typed Islands Parse Faster}}},
  booktitle = {Trends in {{Functional Programming}}},
  author = {Silkensen, Erik and Siek, Jeremy},
  editor = {Loidl, Hans-Wolfgang and Pe{\~n}a, Ricardo},
  year = {2013},
  pages = {69--84},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {This paper addresses the problem of specifying and parsing the syntax of domain-specific languages (DSLs) in a modular, user-friendly way. We want to enable the design of composable DSLs that combine the natural syntax of external DSLs with the easy implementation of internal DSLs. The challenge in parsing these DSLs is that the composition of several languages is likely to contain ambiguities. We present the design of a system that uses a type-oriented variant of island parsing to efficiently parse the syntax of composable DSLs. In particular, we argue that the running time of type-oriented island parsing doesn't depend on the number of DSLs imported. We also show how to use our tool to implement DSLs on top of a host language such as Typed Racket.},
  isbn = {978-3-642-40447-4},
  keywords = {Concrete Syntax,Deductive System,Grammar Rule,Input String,Parse Tree},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{stansiferRomeo2014,
  title = {Romeo},
  booktitle = {Proceedings of the 19th {{ACM SIGPLAN}} International Conference on {{Functional}} Programming - {{ICFP}} '14},
  author = {Stansifer, Paul and Wand, Mitchell},
  year = {2014},
  volume = {49},
  pages = {53--65},
  publisher = {{ACM Press}},
  address = {{New York, New York, USA}},
  doi = {10.1145/2628136.2628162},
  abstract = {Current languages for safely manipulating values with names only support term languages with simple binding syntax. As a result, no tools exist to safely manipulate code written in those languages for which name problems are the most challenging. We address this problem with Romeo, a language that respects {$\alpha$}-equivalence on its values, and which has access to a rich specification language for binding, inspired by attribute grammars. Our work has the complex-binding support of David Herman's {$\lambda$}m, but is a full-fledged binding-safe language like Pure FreshML.},
  isbn = {978-1-4503-2873-9}
}

@article{stansiferRomeoSystemMore2016,
  title = {Romeo: {{A}} System for More Flexible Binding-Safe Programming*},
  shorttitle = {Romeo},
  author = {Stansifer, Paul and Wand, Mitchell},
  year = {2016/ed},
  volume = {26},
  issn = {0956-7968, 1469-7653},
  doi = {10.1017/S0956796816000137},
  abstract = {Current systems for safely manipulating values containing names only support simple binding structures for those names. As a result, few tools exist to safely manipulate code in those languages for which name problems are the most challenging. We address this problem with Romeo, a language that respects {$\alpha$}-equivalence on its values, and which has access to a rich specification language for binding, inspired by attribute grammars. Our work has the complex-binding support of David Herman's {$\lambda$}m
, but is a full-fledged binding-safe language like Pure FreshML.},
  journal = {Journal of Functional Programming},
  language = {en}
}

@inproceedings{steeleOverviewCOMMONLISP1982,
  title = {An {{Overview}} of {{COMMON LISP}}},
  booktitle = {Proceedings of the 1982 {{ACM Symposium}} on {{LISP}} and {{Functional Programming}}},
  author = {Steele, Jr., Guy L.},
  year = {1982},
  pages = {98--107},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/800068.802140},
  abstract = {A dialect of LISP called ``COMMON LISP'' is being cooperatively developed and implemented at several sites. It is a descendant of the MACLISP family of LISP dialects, and is intended to unify the several divergent efforts of the last five years. We first give an extensive history of LISP, particularly of the MACLISP branch, in order to explain in context the motivation for COMMON LISP. We enumerate the goals and non-goals of the language design, discuss the language features of primary interest, and then consider how these features help to meet the expressed goals. Finally, the status (as of May 1982) of six implementations of COMMON LISP is summarized.},
  isbn = {978-0-89791-082-8},
  series = {{{LFP}} '82}
}

@book{sudkampLanguagesMachinesIntroduction1997,
  title = {Languages and {{Machines}}: {{An Introduction}} to the {{Theory}} of {{Computer Science}}},
  shorttitle = {Languages and {{Machines}}},
  author = {Sudkamp, Thomas A.},
  year = {1997},
  publisher = {{Addison-Wesley Longman Publishing Co., Inc.}},
  address = {{Boston, MA, USA}},
  isbn = {978-0-201-82136-9}
}

@article{sujeethDeliteCompilerArchitecture2014,
  title = {Delite: {{A Compiler Architecture}} for {{Performance}}-{{Oriented Embedded Domain}}-{{Specific Languages}}},
  shorttitle = {Delite},
  author = {Sujeeth, Arvind K. and Brown, Kevin J. and Lee, Hyoukjoong and Rompf, Tiark and Chafi, Hassan and Odersky, Martin and Olukotun, Kunle},
  year = {2014},
  month = apr,
  volume = {13},
  pages = {134:1--134:25},
  issn = {1539-9087},
  doi = {10.1145/2584665},
  abstract = {Developing high-performance software is a difficult task that requires the use of low-level, architecture-specific programming models (e.g., OpenMP for CMPs, CUDA for GPUs, MPI for clusters). It is typically not possible to write a single application that can run efficiently in different environments, leading to multiple versions and increased complexity. Domain-Specific Languages (DSLs) are a promising avenue to enable programmers to use high-level abstractions and still achieve good performance on a variety of hardware. This is possible because DSLs have higher-level semantics and restrictions than general-purpose languages, so DSL compilers can perform higher-level optimization and translation. However, the cost of developing performance-oriented DSLs is a substantial roadblock to their development and adoption. In this article, we present an overview of the Delite compiler framework and the DSLs that have been developed with it. Delite simplifies the process of DSL development by providing common components, like parallel patterns, optimizations, and code generators, that can be reused in DSL implementations. Delite DSLs are embedded in Scala, a general-purpose programming language, but use metaprogramming to construct an Intermediate Representation (IR) of user programs and compile to multiple languages (including C++, CUDA, and OpenCL). DSL programs are automatically parallelized and different parts of the application can run simultaneously on CPUs and GPUs. We present Delite DSLs for machine learning, data querying, graph analysis, and scientific computing and show that they all achieve performance competitive to or exceeding C++ code.},
  journal = {ACM Trans. Embed. Comput. Syst.},
  keywords = {code generation,Domain-specific languages,language virtualization,multistage programming},
  number = {4s}
}

@inproceedings{tateEqualitySaturationNew2009,
  title = {Equality Saturation: A New Approach to Optimization},
  shorttitle = {Equality Saturation},
  booktitle = {Proceedings of the 36th Annual {{ACM SIGPLAN}}-{{SIGACT}} Symposium on {{Principles}} of Programming Languages},
  author = {Tate, Ross and Stepp, Michael and Tatlock, Zachary and Lerner, Sorin},
  year = {2009},
  month = jan,
  pages = {264--276},
  publisher = {{Association for Computing Machinery}},
  address = {{Savannah, GA, USA}},
  doi = {10.1145/1480881.1480915},
  abstract = {Optimizations in a traditional compiler are applied sequentially, with each optimization destructively modifying the program to produce a transformed program that is then passed to the next optimization. We present a new approach for structuring the optimization phase of a compiler. In our approach, optimizations take the form of equality analyses that add equality information to a common intermediate representation. The optimizer works by repeatedly applying these analyses to infer equivalences between program fragments, thus saturating the intermediate representation with equalities. Once saturated, the intermediate representation encodes multiple optimized versions of the input program. At this point, a profitability heuristic picks the final optimized program from the various programs represented in the saturated representation. Our proposed way of structuring optimizers has a variety of benefits over previous approaches: our approach obviates the need to worry about optimization ordering, enables the use of a global optimization heuristic that selects among fully optimized programs, and can be used to perform translation validation, even on compilers other than our own. We present our approach, formalize it, and describe our choice of intermediate representation. We also present experimental results showing that our approach is practical in terms of time and space overhead, is effective at discovering intricate optimization opportunities, and is effective at performing translation validation for a realistic optimizer.},
  isbn = {978-1-60558-379-2},
  keywords = {compiler optimization,equality reasoning,intermediate representation},
  series = {{{POPL}} '09}
}

@article{tobin-hochstadtExtensiblePatternMatching2011,
  title = {Extensible {{Pattern Matching}} in an {{Extensible Language}}},
  author = {{Tobin-Hochstadt}, Sam},
  year = {2011},
  month = jun,
  keywords = {Computer Science - Programming Languages},
  language = {en}
}

@inproceedings{tobin-hochstadtLanguagesLibraries2011,
  title = {Languages {{As Libraries}}},
  booktitle = {Proceedings of the {{32Nd ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {{Tobin-Hochstadt}, Sam and {St-Amour}, Vincent and Culpepper, Ryan and Flatt, Matthew and Felleisen, Matthias},
  year = {2011},
  pages = {132--141},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1993498.1993514},
  abstract = {Programming language design benefits from constructs for extending the syntax and semantics of a host language. While C's string-based macros empower programmers to introduce notational shorthands, the parser-level macros of Lisp encourage experimentation with domain-specific languages. The Scheme programming language improves on Lisp with macros that respect lexical scope.  The design of Racket---a descendant of Scheme---goes even further with the introduction of a full-fledged interface to the static semantics of the language. A Racket extension programmer can thus add constructs that are indistinguishable from "native" notation, large and complex embedded domain-specific languages, and even optimizing transformations for the compiler backend. This power to experiment with language design has been used to create a series of sub-languages for programming with first-class classes and modules, numerous languages for implementing the Racket system, and the creation of a complete and fully integrated typed sister language to Racket's untyped base language. This paper explains Racket's language extension API via an implementation of a small typed sister language. The new language provides a rich type system that accommodates the idioms of untyped Racket. Furthermore, modules in this typed language can safely exchange values with untyped modules. Last but not least, the implementation includes a type-based optimizer that achieves promising speedups. Although these extensions are complex, their Racket implementation is just a library, like any other library, requiring no changes to the Racket implementation.},
  isbn = {978-1-4503-0663-8},
  keywords = {extensible languages,macros,modules,typed racket},
  series = {{{PLDI}} '11}
}

@article{vantangTighterBoundDeterminization2009,
  title = {A {{Tighter Bound}} for the {{Determinization}} of {{Visibly Pushdown Automata}}},
  author = {Van Tang, Nguyen},
  year = {2009},
  month = nov,
  volume = {10},
  pages = {62--76},
  issn = {2075-2180},
  doi = {10.4204/EPTCS.10.5},
  abstract = {Visibly pushdown automata (VPA), introduced by Alur and Madhusuan in 2004, is a subclass of pushdown automata whose stack behavior is completely determined by the input symbol according to a fixed partition of the input alphabet. Since its introduce, VPAs have been shown to be useful in various context, e.g., as specification formalism for verification and as automaton model for processing XML streams. Due to high complexity, however, implementation of formal verification based on VPA framework is a challenge. In this paper we consider the problem of implementing VPA-based model checking algorithms. For doing so, we first present an improvement on upper bound for determinization of VPA. Next, we propose simple on-the-fly algorithms to check universality and inclusion problems of this automata class. Then, we implement the proposed algorithms in a prototype tool. Finally, we conduct experiments on randomly generated VPAs. The experimental results show that the proposed algorithms are considerably faster than the standard ones.},
  archivePrefix = {arXiv},
  eprint = {0911.3275},
  eprinttype = {arxiv},
  journal = {Electronic Proceedings in Theoretical Computer Science},
  keywords = {Computer Science - Formal Languages and Automata Theory,Computer Science - Logic in Computer Science,F.4.1,F.4.3}
}

@inproceedings{vanwykContextawareScanningParsing2007,
  title = {Context-Aware {{Scanning}} for {{Parsing Extensible Languages}}},
  booktitle = {Proceedings of the 6th {{International Conference}} on {{Generative Programming}} and {{Component Engineering}}},
  author = {Van Wyk, Eric R. and Schwerdfeger, August C.},
  year = {2007},
  pages = {63--72},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1289971.1289983},
  abstract = {This paper introduces new parsing and context-aware scanning algorithms in which the scanner uses contextual information to disambiguate lexical syntax. The parser uses a slightly modified LR-style algorithm that passes to the scanner the set of valid symbols that the scanner may return at that point in parsing. This set is those terminals whose entries in the parse table for the current parse state are shift, reduce, or accept, but not error. The scanner then only returns tokens in this set. An analysis is given that can statically verify that the scanner will never return more than one token for a single input. Context-aware scanning is especially useful when parsing and scanning extensible languages in which domain specific languages can be embedded. It has been used in extensible versions of Java 1.4 and ANSI C. We illustrate this approach with a declarative specification of a subset of Java and extensions that embed SQL queries and Boolean expression tables into Java.},
  isbn = {978-1-59593-855-8},
  keywords = {context-aware scanning,extensible languages},
  series = {{{GPCE}} '07}
}

@article{vanwykSilverExtensibleAttribute2010,
  title = {Silver: {{An}} Extensible Attribute Grammar System},
  shorttitle = {Silver},
  author = {Van Wyk, Eric and Bodin, Derek and Gao, Jimin and Krishnan, Lijesh},
  year = {2010},
  month = jan,
  volume = {75},
  pages = {39--54},
  issn = {0167-6423},
  doi = {10.1016/j.scico.2009.07.004},
  abstract = {Attribute grammar specification languages, like many domain-specific languages, offer significant advantages to their users, such as high-level declarative constructs and domain-specific analyses. Despite these advantages, attribute grammars are often not adopted to the degree that their proponents envision. One practical obstacle to their adoption is a perceived lack of both domain-specific and general purpose language features needed to address the many different aspects of a problem. Here we describe Silver, an extensible attribute grammar specification system, and show how it can be extended with general purpose features such as pattern matching and domain-specific features such as collection attributes and constructs for supporting data-flow analysis of imperative programs. The result is an attribute grammar specification language with a rich set of language features. Silver is implemented in itself by a Silver attribute grammar and utilizes forwarding to implement the extensions in a cost-effective manner.},
  journal = {Science of Computer Programming},
  keywords = {Attribute grammars,Extensible compilers,Extensible languages,Forwarding,Silver attribute grammar system},
  number = {1},
  series = {Special {{Issue}} on {{ETAPS}} 2006 and 2007 {{Workshops}} on {{Language Descriptions}}, {{Tools}}, and {{Applications}} ({{LDTA}} '06 and '07)}
}

@article{veldhuizenUsingTemplateMetaprograms1995a,
  title = {Using {{C}}++ Template Metaprograms},
  author = {Veldhuizen, Todd},
  year = {1995},
  volume = {7},
  pages = {36--43},
  journal = {C++ Report},
  number = {4}
}

@inproceedings{wanFunctionalReactiveProgramming2000,
  title = {Functional {{Reactive Programming}} from {{First Principles}}},
  booktitle = {Proceedings of the {{ACM SIGPLAN}} 2000 {{Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Wan, Zhanyong and Hudak, Paul},
  year = {2000},
  pages = {242--252},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/349299.349331},
  abstract = {Functional Reactive Programming, or FRP, is a general framework for programming hybrid systems in a high-level, declarative manner. The key ideas in FRP are its notions of behaviors and events. Behaviors are time-varying, reactive values, while events are time-ordered sequences of discrete-time event occurrences. FRP is the essence of Fran, a domain-specific language embedded in Haskell for programming reactive animations, but FRP is now also being used in vision, robotics and other control systems applications. 
In this paper we explore the formal semantics of FRP and how it
relates to an implementation based on streams that represent (and therefore only approximate) continuous behaviors. We show that, in the limit as the sampling interval goes to zero, the implementation is faithful to the formal, continuous semantics, but only when certain constraints on behaviors are observed. We explore the nature of these constraints, which vary amongst the FRP primitives. Our results show both the power and limitations of this approach to language design and implementation. As an example of a limitation, we show that streams are incapable of representing instantaneous predicate events over behaviors.},
  isbn = {978-1-58113-199-4},
  series = {{{PLDI}} '00}
}

@book{webberModernProgrammingLanguages2003,
  title = {Modern {{Programming Languages}}: {{A Practical Introduction}}},
  shorttitle = {Modern {{Programming Languages}}},
  author = {Webber, Adam Brooks},
  year = {2003},
  publisher = {{Franklin, Beedle \& Associates}},
  abstract = {Typical undergraduate CS/CE majors have a practical orientation: they study computing because they like programming and are good at it. This book has strong appeal to this core student group. There is more than enough material for a semester-long course. The challenge for a course in programming language concepts is to help practical ......},
  googlebooks = {RUd5QgAACAAJ},
  isbn = {978-1-887902-76-2},
  keywords = {Computers / General},
  language = {en}
}

@article{youngerRecognitionParsingContextFree1967,
  title = {Recognition and Parsing of Context-Free Languages in Time $n^3$},
  author = {Younger, Daniel H.},
  year = {1967},
  month = feb,
  volume = {10},
  pages = {189-208},
  issn = {0019-9958},
  doi = {10.1016/S0019-9958(67)80007-X},
  abstract = {A recognition algorithm is exhibited whereby an arbitrary string over a given vocabulary can be tested for containment in a given context-free language. A special merit of this algorithm is that it is completed in a number of steps proportional to the âcubeâ of the number of symbols in the tested string. As a byproduct of the grammatical analysis, required by the recognition algorithm, one can obtain, by some additional processing not exceeding the âcubeâ factor of computational complexity, a parsing matrixâa complete summary of the grammatical structure of the sentence. It is also shown how, by means of a minor modification of the recognition algorithm, one can obtain an integer representing the ambiguity of the sentence, i.e., the number of distinct ways in which that sentence can be generated by the grammar. The recognition algorithm is then simulated on a Turing Machine. It is shown that this simulation likewise requires a number of steps proportional to only the âcubeâ of the test string length.},
  journal = {Information and Control},
  number = {2}
}


